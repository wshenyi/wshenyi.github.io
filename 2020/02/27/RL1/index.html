<!DOCTYPE html>
<html lang="">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_16x16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wshenyi.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":300,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Refer from Reinforcement Learning An Introduction (2nd) and Reinforcement Learning specialization on Coursera by University of Alberta &amp; Alberta Machine Intelligence Institute  The K-Armed Band">
<meta property="og:type" content="article">
<meta property="og:title" content="RL1：K-armed Bandits Problem">
<meta property="og:url" content="https://wshenyi.github.io/2020/02/27/RL1/index.html">
<meta property="og:site_name" content="Godway&#39;s Notebook">
<meta property="og:description" content="Refer from Reinforcement Learning An Introduction (2nd) and Reinforcement Learning specialization on Coursera by University of Alberta &amp; Alberta Machine Intelligence Institute  The K-Armed Band">
<meta property="og:locale">
<meta property="og:image" content="https://wshenyi.github.io/2020/02/27/RL1/1.png">
<meta property="og:image" content="https://wshenyi.github.io/2020/02/27/RL1/2.png">
<meta property="og:image" content="https://wshenyi.github.io/2020/02/27/RL1/3.png">
<meta property="og:image" content="https://wshenyi.github.io/2020/02/27/RL1/5.png">
<meta property="og:image" content="https://wshenyi.github.io/2020/02/27/RL1/4.png">
<meta property="og:image" content="https://wshenyi.github.io/2020/02/27/RL1/6.png">
<meta property="og:image" content="https://wshenyi.github.io/2020/02/27/RL1/7.png">
<meta property="og:image" content="https://wshenyi.github.io/2020/02/27/RL1/8.png">
<meta property="og:image" content="https://wshenyi.github.io/2020/02/27/RL1/9.png">
<meta property="og:image" content="https://wshenyi.github.io/2020/02/27/RL1/10.png">
<meta property="article:published_time" content="2020-02-27T17:35:49.000Z">
<meta property="article:modified_time" content="2020-03-17T15:19:28.000Z">
<meta property="article:author" content="Godway">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wshenyi.github.io/2020/02/27/RL1/1.png">

<link rel="canonical" href="https://wshenyi.github.io/2020/02/27/RL1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'default'
  };
</script>

  <title>RL1：K-armed Bandits Problem | Godway's Notebook</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Godway's Notebook</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">wsy</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/WSHENYI/WSHENYI.github.io" class="github-corner" title="Star me on GitHub" aria-label="Star me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="default">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2020/02/27/RL1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          RL1：K-armed Bandits Problem
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-27 12:35:49" itemprop="dateCreated datePublished" datetime="2020-02-27T12:35:49-05:00">2020-02-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-17 11:19:28" itemprop="dateModified" datetime="2020-03-17T11:19:28-04:00">2020-03-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>

<blockquote>
<p>Refer from <em>Reinforcement Learning An Introduction (2nd)</em> and <em>Reinforcement Learning specialization on Coursera by University of Alberta &amp; Alberta Machine Intelligence Institute</em></p>
</blockquote>
<h2 id="The-K-Armed-Bandit-Problem"><a href="#The-K-Armed-Bandit-Problem" class="headerlink" title="The K-Armed Bandit Problem"></a>The K-Armed Bandit Problem</h2><p>In the k-armed bandit problem, we have an <strong>agent</strong> who choooses between “k” <strong>action</strong> and recieves a <strong>reward</strong> based on the action it chooses. The <strong>goal</strong> of the agent is to maximize the expected total reward over some time period, or <strong>time steps</strong>.</p>
<p>In a word, <strong>decision making under uncertainty</strong> can be formalized by the <strong>k-armed bandit problem</strong></p>
<h3 id="Action-Values"><a href="#Action-Values" class="headerlink" title="Action Values"></a>Action Values</h3><p>the value of an action is the <strong>expected reward</strong> when that action is taken<br>$$<br>\begin{eqnarray*}<br>q_*(a) &amp;\doteq&amp; \mathbb{E}[R_t\ | \ A_t = a],\ \forall a \in \mathbf{A}\<br>&amp;=&amp;\sum_rp(r|a)r\tag{1.1}<br>\end{eqnarray*}<br>$$<br>To clearly explain this, the reward of an action may not be fixed so the reward may comfirm to a uniform distribution or normal distribution. Here is an example to illustrate this process</p>
<img src="/2020/02/27/RL1/1.png" class="">  

<h3 id="Action-Selection"><a href="#Action-Selection" class="headerlink" title="Action Selection"></a>Action Selection</h3><p>Remember that the agent’s <strong>goal</strong> is to <strong>maximize the expected total amount of reward it recieves</strong>. We can call this procedure the <em>argmax</em> (the argument “a” which maximizes function $q_*$)<br>$$\mathop{argmax}<em>a \ q</em>*(a)$$<br>It is clear that if you know the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the <strong>action with highest value</strong>.</p>
<p>Thus, the problem here is that the agent do not know the action values with certainty. It only have to estimate the action values. But how to estimate action-values?</p>
<hr>
<h2 id="What-to-Learn-Estimating-Action-Values"><a href="#What-to-Learn-Estimating-Action-Values" class="headerlink" title="What to Learn? Estimating Action Values"></a>What to Learn? Estimating Action Values</h2><h3 id="Sample-Average"><a href="#Sample-Average" class="headerlink" title="Sample-Average"></a>Sample-Average</h3><p>One way to estimate $q_*$ is to compute a <strong>sample-average</strong>. Here we denote the estimated value of action $a$ at time step $t$ as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q_*(a)$.</p>
<p>$$Q_t(a) \doteq \frac{\sum_{i=1}^{t-1}R_i}{t-1}\tag{1.2}$$</p>
<p>Notice that we use $t-1$ because we are calculating $Q_t(a)$ with history prior to $t$. And to be clear, $t$ has <strong>separate corresponding value for each action</strong>.<br>Here is an example of medical trail.</p>
<img src="/2020/02/27/RL1/2.png" class="">  
<p>And we can simplify $Q_t$ by extending it in <strong>incremental update rule</strong></p>
<p>$$<br>\begin{eqnarray*}<br>Q_{t+1}<br>&amp;=&amp;\frac{1}{t}\sum_{i=1}^tR_i\<br>&amp;=&amp;\frac{1}{t}(R_t+(t-1)\frac{1}{t-1}\sum_{i=1}^{t-1}R_i)\<br>&amp;=&amp;\frac{1}{t}(R_t+(t-1)Q_t)\<br>&amp;=&amp;Q_t+\frac{1}{t}(R_t-Q_t)\tag{1.3}<br>\end{eqnarray*}<br>$$</p>
<p>To generalize it, we replace $\frac{1}{t}$ with $\alpha_t$ to represent stepsize and it’s easy to know $\alpha_t \in [0,1]$</p>
<p>$$Q_{t+1}=Q_t+\alpha_t(R_t-Q_t)\tag{1.4}$$</p>
<p>And it can also express as below</p>
<p>$$NewEstimate \leftarrow OldEstimate\ +\  StepSize(Target\ -\ OldEstimate)$$</p>
<p>The expression $(Target-OldEstimate)$ is an <strong>error</strong> in the estimate. It is reduced by taking a step toward the “Target.” <strong>The target is presumed to indicate a desirable direction in which to move, though it may be noisy.</strong> In the case above, for example, the target is the nth reward.</p>
<h3 id="Nonstationary-Problem"><a href="#Nonstationary-Problem" class="headerlink" title="Nonstationary Problem"></a>Nonstationary Problem</h3><p>What if the reward distribution of each action is <strong>changing over time</strong>? In this situation, what we learn form history may not correctly represent current state due to changing of reward distribution. In such cases it makes sense to give more weight to recent rewards than to long-past rewards because recent information reflect changing more accurate. To do so, we can use a <strong>fixed stepsize</strong>,like 0.5, instead of $\frac{1}{t}$. This can lead to decaying exponently past reward.</p>
<p>$$Q_{t+1}=Q_t+\alpha_t(R_t-Q_t)=\alpha_tR_t+(1-\alpha_t)Q_t\tag{1.5}$$</p>
<p>As $\alpha_t$ increases, the more $R_t$ contributes to $Q_{t+1}$ and the less for $Q_t$.</p>
<h3 id="Greedy-Action"><a href="#Greedy-Action" class="headerlink" title="Greedy Action"></a>Greedy Action</h3><p>At each time step, the agent could alway choose the action with the highest action value estimated by history. And this is called <strong>greedy action</strong>.</p>
<p>$$a_g = \mathop{argmax}_a\ \ Q(a)$$</p>
<p>Seemingly we can through greedy action to archieve our goal, maxmizing expected total reward. But there is still a problem exists: how does the agent know its estimation about each action is correctly approximating to the real distribution? Greedy action is greatly impact by initial action value. Why does the agent know other actions are worse than greedy action even trying them only a few times?</p>
<p>I think the agent should spend some time trying other actions instead of greedy action. And this come to the <strong>exploration and exploitation tradeoff</strong></p>
<hr>
<h2 id="Exploration-vs-Exploitation-Tradeoff"><a href="#Exploration-vs-Exploitation-Tradeoff" class="headerlink" title="Exploration vs. Exploitation Tradeoff"></a>Exploration vs. Exploitation Tradeoff</h2><h3 id="Exploration-and-Exploitation-Dilemma"><a href="#Exploration-and-Exploitation-Dilemma" class="headerlink" title="Exploration and Exploitation Dilemma"></a>Exploration and Exploitation Dilemma</h3><ul>
<li>Exploration: <strong>improve</strong> knowledge for <strong>long-term</strong> benefit</li>
<li>Exploitation: <strong>exploit</strong> knowledge for <strong>short-term</strong> benefit</li>
</ul>
<p>The dilemma means: How do we choose when to exploit and when to explore? When we <strong>explore</strong>, we get more accurate estimates of our values. When we <strong>exploit</strong>, we might get more reward. We cannot however choose to do both simultaneously.</p>
<h3 id="Epsilon-Greedy-Action-Selection"><a href="#Epsilon-Greedy-Action-Selection" class="headerlink" title="Epsilon-Greedy Action Selection"></a>Epsilon-Greedy Action Selection</h3><p>One very simple method for choosing between exploration and exploitation is to <strong>choose randomly</strong>. We could choose to exploit most of the time with a small chance of exploring. For instance, we could roll a dice.  </p>
<p>This is called <strong>$\epsilon$-greedy action selection</strong>. We formulize it as below, where $\epsilon \in [0,1]$</p>
<p>$$<br>A_t\leftarrow<br>\begin{cases}<br>\mathop{argmax}_a Q_t(a)\ \ \ \ \ \ \ \ 1-\epsilon\ \<br>a\sim Uniform(\mathbf{A})\ \ \ \ \ \ \epsilon<br>\end{cases}<br>$$</p>
<img src="/2020/02/27/RL1/3.png" class="">  

<p>Let’s evaluate on 10-armed testbed  </p>
<img src="/2020/02/27/RL1/5.png" class="">  
<p>We run each agent with different $\epsilon$ for 2000 independent runs, each run for 1000 time steps. Then we average 2000 runs to get the learning algorithm’s average behavior.  </p>
<img src="/2020/02/27/RL1/4.png" class="">  

<p>The advantage of $\epsilon$-greedy over greedy methods depends on the task. If the reward variance is large, like 10, then more explorations to detect best action is important. More detections give the agent more confidence to pick up optimal action.</p>
<p>On the other hand, if the reward variance is zero, then greedy method would know the true value of each action after trying it once. Seemingly, in this case, greedy method would soon find the greedy action and it might perform best. But, this can not be guaranteed under nonstationary situations. Nonstation means the true values of the actions changed over time. In this case, exploration is needed to ensure that one of the nongreedy actions has not changed to become better than the greedy one.</p>
<h3 id="Optimistic-Initial-Values"><a href="#Optimistic-Initial-Values" class="headerlink" title="Optimistic Initial Values"></a>Optimistic Initial Values</h3><p><strong>Optimistic initial values</strong> encourage <strong>early exploration</strong>. In real programing, don’t intialize all acion values defaut to 0. Here is an example to show algorithm’s performance by using this method.  </p>
<img src="/2020/02/27/RL1/6.png" class="">  
<p>Limitation of optimistic inital values:</p>
<ul>
<li>Optimistic intial values <strong>only drive early exploration</strong></li>
<li>They are not will suited for <strong>nonstationary problems</strong></li>
<li>We may not know what the <strong>optimistic values</strong> should be</li>
</ul>
<h3 id="Upper-Confidence-Bound-UCB-Action-Selection"><a href="#Upper-Confidence-Bound-UCB-Action-Selection" class="headerlink" title="Upper-Confidence Bound (UCB) Action Selection"></a>Upper-Confidence Bound (UCB) Action Selection</h3><p>$\epsilon$-greedy action selection forces the non-greedy actions to be tried, but indiscriminately and <strong>uniformly</strong>, with no preference for those that are nearly greedy or particularly uncertain. It would be better to select among the non-greedy actions according to their <strong>potential for actually being optimal</strong>, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates.</p>
<p>In <strong>upper-confidence bound</strong>(UCB) action selection, we follow the principle of optimism in the face of uncertainty. This simply means that if we are uncertain about something, we should optimistically assume that it is good.</p>
<p>For instance, say we have these three actions with associated uncertainties. The larger confidence interval means bigger uncertainies. So the agent optimistically picks the action that has the highest upper bound, action 1.</p>
<img src="/2020/02/27/RL1/7.png" class="">  
<p>After that, this time, we should select action 2.  </p>
<img src="/2020/02/27/RL1/8.png" class="">  
<p>The key idea in UCB is that it uses <strong>uncertainty</strong> or <strong>variance</strong> (sqrt root) in the value estimation for balancing exploration and exploitation.</p>
<p>$$A_t \doteq \mathop{argmax}_a [Q_t(a)+c\sqrt{\frac{\ln{t}}{N_t(a)}}\ ]$$</p>
<p>We can clearly see here how UCB combines exploration and exploitation. The first term in the sum represents the exploitation part, and the second term represents the exploration part. The constant parameter $c$ is used to scale the extent of exploration. $N_t(a)$ denotes the number of times that action $a$ has been selected prior to time $t$.  </p>
<img src="/2020/02/27/RL1/9.png" class="">  
<p>The limitation of UCB is <strong>not practical for very huge state spaces</strong>.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>The gap between k-armed bandit problem and the full reinforcement learning problem:</p>
<ol>
<li>there is <strong>no association</strong> between actions and situations(or <strong>states</strong>). Each selection of actions is <strong>non-contextual</strong> or independent.</li>
<li>thers is <strong>no policy</strong>, which designate actions at each situation(or <strong>state</strong>).</li>
</ol>
<p>we summarize a complete learning curve by its average value over the 1000 steps; this value is proportional to the area under the learning curve. This kind of graph is called a <strong>parameter study</strong>. </p>
<img src="/2020/02/27/RL1/10.png" class="">

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/02/25/Game2/" rel="prev" title="博弈论2：纯策略纳什均衡">
      <i class="fa fa-chevron-left"></i> 博弈论2：纯策略纳什均衡
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/02/28/RL2/" rel="next" title="RL2：Markov Decision Processes">
      RL2：Markov Decision Processes <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-K-Armed-Bandit-Problem"><span class="nav-number">1.</span> <span class="nav-text">The K-Armed Bandit Problem</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Action-Values"><span class="nav-number">1.1.</span> <span class="nav-text">Action Values</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Action-Selection"><span class="nav-number">1.2.</span> <span class="nav-text">Action Selection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-to-Learn-Estimating-Action-Values"><span class="nav-number">2.</span> <span class="nav-text">What to Learn? Estimating Action Values</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Sample-Average"><span class="nav-number">2.1.</span> <span class="nav-text">Sample-Average</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Nonstationary-Problem"><span class="nav-number">2.2.</span> <span class="nav-text">Nonstationary Problem</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Greedy-Action"><span class="nav-number">2.3.</span> <span class="nav-text">Greedy Action</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Exploration-vs-Exploitation-Tradeoff"><span class="nav-number">3.</span> <span class="nav-text">Exploration vs. Exploitation Tradeoff</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Exploration-and-Exploitation-Dilemma"><span class="nav-number">3.1.</span> <span class="nav-text">Exploration and Exploitation Dilemma</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Epsilon-Greedy-Action-Selection"><span class="nav-number">3.2.</span> <span class="nav-text">Epsilon-Greedy Action Selection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimistic-Initial-Values"><span class="nav-number">3.3.</span> <span class="nav-text">Optimistic Initial Values</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Upper-Confidence-Bound-UCB-Action-Selection"><span class="nav-number">3.4.</span> <span class="nav-text">Upper-Confidence Bound (UCB) Action Selection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">4.</span> <span class="nav-text">Summary</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Godway"
      src="/images/avatar.ico">
  <p class="site-author-name" itemprop="name">Godway</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">61</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="/475865836@qq.com" title="E-Mail → 475865836@qq.com"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://randool.cn/" title="https:&#x2F;&#x2F;randool.cn" rel="noopener" target="_blank">Randool</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://me.csdn.net/Smile_coderrr" title="https:&#x2F;&#x2F;me.csdn.net&#x2F;Smile_coderrr" rel="noopener" target="_blank">Smile_coderrr</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Godway</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
