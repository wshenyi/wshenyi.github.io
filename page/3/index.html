<!DOCTYPE html>
<html lang="">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_16x16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wshenyi.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":300,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Godway&#39;s Notebook">
<meta property="og:url" content="https://wshenyi.github.io/page/3/index.html">
<meta property="og:site_name" content="Godway&#39;s Notebook">
<meta property="og:locale">
<meta property="article:author" content="Godway">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://wshenyi.github.io/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'default'
  };
</script>

  <title>Godway's Notebook</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Godway's Notebook</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">wsy</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/WSHENYI/WSHENYI.github.io" class="github-corner" title="Star me on GitHub" aria-label="Star me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2022/01/20/Arch2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/20/Arch2/" class="post-title-link" itemprop="url">Computer Architecture - A Quantitative Approach, 6th [A.1 - A.5]</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-20 11:34:35" itemprop="dateCreated datePublished" datetime="2022-01-20T11:34:35-05:00">2022-01-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-03-03 18:48:35" itemprop="dateModified" datetime="2022-03-03T18:48:35-05:00">2022-03-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Architecture/" itemprop="url" rel="index"><span itemprop="name">Architecture</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>Author: John L. Hennessy, David A. Patterson</p>
</blockquote>
<p>Chapter A: Instruction Set Principles</p>
<hr>
<h2 id="A-1-Introduction"><a href="#A-1-Introduction" class="headerlink" title="A.1 Introduction"></a>A.1 Introduction</h2><p>Instruction set architecture (ISA) —- the portion of the computer visible to the programmer or compiler writer.</p>
<p>Recent 80x86 microprocessors, including all the Intel Core microprocessors build in the past decades, use hardware to translate from 80x86 instructions to RISC-like instructions and then execute the translated operations inside the chip. They maintain the illusion of 80x86 architecture to the programmer while allowing the computer designer to implement a RISC-style processor for performance.</p>
<h2 id="A-2-lassifying-Instruction-Set-Architectures"><a href="#A-2-lassifying-Instruction-Set-Architectures" class="headerlink" title="A.2 lassifying Instruction Set Architectures"></a>A.2 lassifying Instruction Set Architectures</h2><p>The type of internal storage in a processor is the most basic differentiation. They major choices are a <strong>stack</strong>, an <strong>accumulator</strong>, or <strong>a set of registers</strong>.</p>
<p>Operands may be named <strong>explicitly</strong> or <strong>implicitly</strong>: The operands in a <em>stack</em> architecture are implicitly on the top of the stack, and in an <em>accumulator</em> architecture one operand is implicitly the accumulator, and the <em>general-purpose register</em> (GPR)architectures have only explicit operands – either registers or memory locations.</p>
<img src="/2022/01/20/Arch2/1.png" class="">

<p>Three most common types of general-purpose register computers.</p>
<ul>
<li><strong>register-memory architecture</strong>: access memory as part of any instruction</li>
<li><strong>register-register architecure</strong>: also called load-store architecture and it accesses memory only with load and store instructions.</li>
<li><strong>memory-memory architecture</strong>: not found in computers shipping today, keeps all operands in memory.</li>
</ul>
<p>The major reason for the emergence of general-purpose register (GPR) computers are twofold: (1) registers are faster than memory (2) registers are more efficient for a compiler to use than other forms of internal storage.</p>
<p>The answer to the question “how many registers are sufficient?” depends on the effectiveness of the compiler. Most compilers reserve some registers for expression evaluation, use some for parameter passing,and allow the remainder to be allocated to hold variables. This is called ABI.</p>
<p>Two major <em>instruction set characteristics</em> divide GPR architectures.</p>
<ul>
<li>Whether an ALU instruction has two or three operands.</li>
<li>How many of the operands may be memory address in ALU instructions.</li>
</ul>
<img src="/2022/01/20/Arch2/5.png" class="">

<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><img src="/2022/01/20/Arch2/2.png" class="">

<p>These advantages and disadvantages are qualitative and their actual impac depneds on the compiler and implementation strategy. (e.g. A GPR computer with memory operations could easily be ignored by the compiler and used as a load-store computer.)</p>
<h2 id="A-3-Memory-Addressing"><a href="#A-3-Memory-Addressing" class="headerlink" title="A.3 Memory Addressing"></a>A.3 Memory Addressing</h2><h3 id="Interpreting-Memory-Addresses"><a href="#Interpreting-Memory-Addresses" class="headerlink" title="Interpreting Memory Addresses"></a>Interpreting Memory Addresses</h3><p>There are two different convention for <strong>ordering the bytes</strong> within a larger object. <strong>Little Endian</strong> byte order puts least-significant bit at right position in a byte. <strong>Big Endian</strong> byte order puts most-significant bit at left position in a byte.</p>
<p>In many computers, accesses to objects larger than a byte must <strong>aligned</strong>. Misalignent causes hardware complications, because the memroy is typically aligned on a multiple of a word or double-word boundary.</p>
<h3 id="Addressing-Modes"><a href="#Addressing-Modes" class="headerlink" title="Addressing Modes"></a>Addressing Modes</h3><p>Addressing modes specify constants and registers in addition to locations in memroy. The actual memory address specified by the addressing mode is called the <strong>effective addrss</strong>.</p>
<img src="/2022/01/20/Arch2/3.png" class="">

<p>Addressing mode that depend on the program counter, called <em>PC-relative addressing</em>.</p>
<p>Addressing modes have the ability to significantly reduce instrcution counts; they also add to the complexity of building a computer and may increase the average clock cycles per instruction (CPI) of computers that implement those modes. Thus, the usage of various addressing modes is quite important in helping the architect chosse what to include.</p>
<h2 id="A-4-Type-and-Size-of-Operands"><a href="#A-4-Type-and-Size-of-Operands" class="headerlink" title="A.4 Type and Size of Operands"></a>A.4 Type and Size of Operands</h2><p>Usually, the type of an operand is encoding in the <strong>opcode</strong> designates the type of an operand – this is the method used most often. Alternatively, the data can be annoted with <strong>tags</strong> that are interpreted by the hardware.</p>
<p>Usually, the type of an operand – integer, single-precision floating point, character, and so on – effectively gives its size. Common operand types include character (8 bits), half word (16 bits), word (32 bits), single-precision floating point (also 1 word), and double- precision floating point (2 words).</p>
<h2 id="A-5-Operations-in-the-Instruction-Set"><a href="#A-5-Operations-in-the-Instruction-Set" class="headerlink" title="A.5 Operations in the Instruction Set"></a>A.5 Operations in the Instruction Set</h2><p>One rule of thumb across all architectures is that the most widely executed instructions are the simple operations of an instruction set.</p>
<img src="/2022/01/20/Arch2/4.png" class="">

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2022/01/19/R19/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/19/R19/" class="post-title-link" itemprop="url">019 "IGC - The Open Source Intel Graphics Compiler"</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-19 15:31:40" itemprop="dateCreated datePublished" datetime="2022-01-19T15:31:40-05:00">2022-01-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-13 17:20:40" itemprop="dateModified" datetime="2022-02-13T17:20:40-05:00">2022-02-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reviews/" itemprop="url" rel="index"><span itemprop="name">Reviews</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em><strong>Keywards</strong></em> Compiler, GPU</p>
<blockquote>
<p>Author: Anupama Chandrasekhar, Gang Chen, Po-Yu Chen, Wei-Yu Chen, Junjie Gu, Peng Guo, Shruthi Hebbur Prasanna Kumar, Guei-Yuan Lueh, Pankaj Mistry, Wei Pan, Thomas Raoux, and Konrad Trifunovic</p>
</blockquote>
<hr>
<h2 id="What-is-the-Problem"><a href="#What-is-the-Problem" class="headerlink" title="What is the Problem?"></a>What is the Problem?</h2><p>NVIDIA amd the CUDA programming model have dominated the GPU computing space. Intel GPUs have been getting more powerful and are stronger contenders in the graphics and GPGPU space.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>GPUs have become the mainstay for a wide variety of compute intensive and Intel wants to make more real-world impact in general computing. Therefore, the Intel team presents the Intel Graphics Compiler (IGC), a LLVM-based production compiler for Intel HD and Iris graphics. This paper mainly focuss on how they design IR optimization for GPU with LLVM generic IR optimization.</p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ul>
<li>Intel GPUs are also very attractive for heterogeneous computing thanks to their tight integration with the CPU and their ubiquity on nearly every Intel desktop and mobile processor.</li>
<li>A kernel written in <strong>OpenCL C</strong> is first compiled using the Clang compiler into <strong>SPIR-V</strong> (Standard Portable Intermediate Representation) bitcode, which is then converted by IGC’s front-end into <strong>LLVM IR</strong> by <em>unification passes</em>.</li>
</ul>
<h2 id="Key-Insights"><a href="#Key-Insights" class="headerlink" title="Key Insights"></a>Key Insights</h2><ul>
<li>IGC is based on LLVM compiler infrastructure and uses a proprietary backend called the <em>virtual-ISA finalizer</em> for code generation.</li>
<li>IGC provides additional custom optimization passes on IR leverage with LLVM’s generic optimization. (LLVM’s IR optimization passes are mainly designed for CPU)</li>
</ul>
<h2 id="Notable-Design-Details-Strengths"><a href="#Notable-Design-Details-Strengths" class="headerlink" title="Notable Design Details/Strengths"></a>Notable Design Details/Strengths</h2><p>Intel Graphics Compiler consists of three layers:</p>
<ol>
<li>Front-end: supported API =&gt; IGC’s LLVM-based IR</li>
<li>Mid-end: IGC’s LLVM-based IR =&gt; (generic LLVM optimizations, IGC custom optimizations, and code generation) =&gt; virtual-ISA<ul>
<li>Divergence Analysis, Pre-RA Scheduling, Private Memory Promotion, Addressing Mode Selection, Copy Coalescing, SIMD Width Selection</li>
</ul>
</li>
<li>Finalizer: virtual-ISA =&gt; Gen ISA</li>
</ol>
<img src="/2022/01/19/R19/1.png" class="">

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2022/01/18/Arch1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/18/Arch1/" class="post-title-link" itemprop="url">Computer Architecture - A Quantitative Approach, 6th [Chapter 1]</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-18 10:52:44" itemprop="dateCreated datePublished" datetime="2022-01-18T10:52:44-05:00">2022-01-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-15 16:48:24" itemprop="dateModified" datetime="2022-02-15T16:48:24-05:00">2022-02-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Architecture/" itemprop="url" rel="index"><span itemprop="name">Architecture</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h2 id=""><a href="#" class="headerlink" title=""></a></h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2022/01/17/R18/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/17/R18/" class="post-title-link" itemprop="url">018 "Understanding the Energy Consumption of Dynamic Random Access Memories"</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-17 11:44:00" itemprop="dateCreated datePublished" datetime="2022-01-17T11:44:00-05:00">2022-01-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-18 00:59:35" itemprop="dateModified" datetime="2022-01-18T00:59:35-05:00">2022-01-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reviews/" itemprop="url" rel="index"><span itemprop="name">Reviews</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em><strong>Keywards</strong></em> DRAM; Power</p>
<blockquote>
<p>Author: Thomas Vogelsang</p>
</blockquote>
<hr>
<h2 id="What-is-the-Problem"><a href="#What-is-the-Problem" class="headerlink" title="What is the Problem?"></a>What is the Problem?</h2><p>We still miss a model of DRAM power consumption that is both detailed to direct optimization work, but also general enough not to be restricted to an existing DRAM technology or DRAM standard.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>This paper proposse a flexible DRAM power model which uses a description of DRAM architecture, technology and opearation to calculate power useage and verifies it against datasheet values. Using this model, the author evaluates some of the proposed DRAM power reduction schemes.</p>
<h2 id="Contribution-Key-Results"><a href="#Contribution-Key-Results" class="headerlink" title="Contribution / Key Results"></a>Contribution / Key Results</h2><p>Proposed a model of DRAM power consumption.</p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Three most important factors for DRAM cost are:</p>
<ol>
<li>cost of a wafer: cost of a wafer can be kept low if a simple transistor process and few metal levels are used.</li>
<li>yield: it can be optimized by process optimization and by optimization the amount of redundancy.</li>
<li>die area: its optimization is achieved by keeping the array efficiency (ratio of cell area to total die area) as high as possible.</li>
</ol>
<p>High performance DRAMs are optimized for maximum total data rate.</p>
<p>Mobile DRAMs are optimized for low standby current with data rates similiar to commodity DRAM.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2022/01/16/R17/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/16/R17/" class="post-title-link" itemprop="url">017 "Improving GPU Performance via Large Warps and Two-Level Warp Scheduling"</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-16 08:13:14" itemprop="dateCreated datePublished" datetime="2022-01-16T08:13:14-05:00">2022-01-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-16 17:15:58" itemprop="dateModified" datetime="2022-02-16T17:15:58-05:00">2022-02-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reviews/" itemprop="url" rel="index"><span itemprop="name">Reviews</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em><strong>Keywards</strong></em> GPGPU, SIMD, Divergence, Warp Scheduling</p>
<blockquote>
<p>Author: Veynu Narasiman, Michael Shebanow, Chang Joo Lee, Rustam Miftakhutdinov, Onur Mutlu, Yale N. Patt</p>
</blockquote>
<hr>
<h2 id="What-is-the-Problem"><a href="#What-is-the-Problem" class="headerlink" title="What is the Problem?"></a>What is the Problem?</h2><p>Altough GPUs achieve Thread Level Parallelism (TLP) through warp execution and concurrently executing manny warps on single GPU core (SM), the computational resources on GPU are still underutilized.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Threads on GPU are grouped into fixed-size SIMD batched known as warps and many warps are concurrently executed on a single GPU core. Despite this, computationaly resources on GPU cores are still underutillized. To further improve GPU performance, the authors propose two independent ideas: the large warp microarchitecture and two-level warp scheduling.</p>
<h2 id="Contribution-Key-Results"><a href="#Contribution-Key-Results" class="headerlink" title="Contribution / Key Results"></a>Contribution / Key Results</h2><p>Their mechanisms signifi-cantly improve computational resource utilization, resulting in 19.1% performance improvement over traditional GPU cores on a set of general purpose parallel applications.</p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>GPUs expliot TLP in two major way:</p>
<ol>
<li>threads executing in fixed-size batches known as warps.</li>
<li>concurrently execute many warps on a singe core to hide execution latency.</li>
</ol>
<p>Two resouce for underutilization of computational resource:</p>
<ol>
<li>branch divergence due to conditional branch in the code.</li>
<li>unable to effectively hide latency of long latency operations.</li>
</ol>
<h2 id="Key-Insights"><a href="#Key-Insights" class="headerlink" title="Key Insights"></a>Key Insights</h2><ul>
<li>Large warp microarchitecture</li>
<li>Two-level warp scheduling</li>
</ul>
<h2 id="Limitations-Weaknesses"><a href="#Limitations-Weaknesses" class="headerlink" title="Limitations/Weaknesses"></a>Limitations/Weaknesses</h2><ul>
<li>The assumptions about default warp scheduling on GPU is round-robin may not be correct.</li>
<li>For data intensive application, the two level round-robin scheduling may not bring much performance improvement.</li>
<li>The hardware cost for implmentation their methods may not worth it because small improvemence is only improved on specific applications.</li>
</ul>
<h2 id="Open-Questions-Where-to-go-from-here"><a href="#Open-Questions-Where-to-go-from-here" class="headerlink" title="Open Questions [Where to go from here?]"></a>Open Questions [Where to go from here?]</h2><ul>
<li>How their methods apply to three dimentional grid?</li>
<li>In practice, the active mask may not be as sparse as they shown in the paper.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2022/01/15/R16/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/15/R16/" class="post-title-link" itemprop="url">016 "General-Purpose Graphics Processor Architectures"</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-15 08:11:53" itemprop="dateCreated datePublished" datetime="2022-01-15T08:11:53-05:00">2022-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-16 17:15:52" itemprop="dateModified" datetime="2022-02-16T17:15:52-05:00">2022-02-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reviews/" itemprop="url" rel="index"><span itemprop="name">Reviews</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em><strong>Keywards</strong></em> GPGPU, computer architecture</p>
<blockquote>
<p>Author: Tor M. Aamodt, Wilson Wai Lun Fung, Timothy G. Rogers</p>
</blockquote>
<hr>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Over succeeding generations of products, GPU manufacturer’s have refined the GPU architecture and programming model to <strong>increase flexibility</strong> while simultaneously <strong>improving energy efficiency</strong>.</p>
<p>A typical system containing GPU is divided into two forms: 1. discrete CPU and GPU; 2. integrated CPU and GPU</p>
<img src="/2022/01/15/R16/1.png" class="">

<p>For distrete GPU, the DRAM technology used for CPU and GPU is different(<strong>DDR</strong> for CPU vs. <strong>GDDR</strong> for GPU). DDR for <em>Double Data Rate</em> and GDDR for Graphics DDR. The CPU DRAM is typically optimized for <strong>low latency access</strong> whereas the GPU DRAM is optimized for <strong>high throughput</strong>. For integrated GPU, as they are often found on low-powper mobile device, the shared DRAM memory it uses is often optimized for <strong>low power</strong> such as <strong>LPDDR</strong>.</p>
<p>A generic model GPU architecture, <strong>SIMT</strong></p>
<img src="/2022/01/15/R16/2.png" class="">

<p>GPUs can obtain improved performance per unit area vs. superscalar out-of-order CPUs on highly parallel workloads by dedicating a larger fraction of their die area to arithmetic logic units and correspondingly less area to control logic. Below is an analytical model-based analysis of the performance tradeoff between multicore (MC) CPU architectures and multithreaded (MT) architectures such as GPU.</p>
<img src="/2022/01/15/R16/3.png" class="">

<p>As the number of threads increases from the middle point, performance increases with the ability of multithreading to hide long off-chip latency. GPUs are designed to tolerate frequent cache misses by employing multithreading.</p>
<h2 id="Programming-Model"><a href="#Programming-Model" class="headerlink" title="Programming Model"></a>Programming Model</h2><p>A well-known operation <strong>SAXPY</strong> represent for <em>single-precision scalar value <strong>A</strong> times vector value <strong>X</strong> plus vector value <strong>Y</strong></em></p>
<p>Currently, NVIDIA warps consists of 32 threads while AMD wavefronts consist of 64 threads.</p>
<p>NVIDIA has introduced <strong>Unified Memory</strong>, which transparently updates GPU memory from CPU memory and CPU memory from GPU memory.</p>
<p>NVIDIA’s PTX(Parallel Thread Execution ISA) is many ways similar to a standard reduced instruction set computer (RISC) instruction set architecture. It also shares a similarity to the intermediate representations (IR) used within optimizing compilers.</p>
<p>Before running PTX code on the GPU it is necessary to compile PTX down to the actual instruction set architecture supported by the hardware. NVIDIA calls this level <strong>SASS</strong> which is short for “Streaming ASSembler”. The process ofconverting from PTX to SASS can be accomplished either by the GPU driver or a stand-alone program called <em>ptxas</em> provided with NVIDIA’s CUDA Toolkit.</p>
<h2 id="The-SIMT-core-Instruction-and-Register-Data-Flow"><a href="#The-SIMT-core-Instruction-and-Register-Data-Flow" class="headerlink" title="The SIMT core: Instruction and Register Data Flow"></a>The SIMT core: Instruction and Register Data Flow</h2><p>An <strong>approximating</strong> microarchitecture of a generic GPGPU core.</p>
<img src="/2022/01/15/R16/4.png" class="">

<p>The pipeline can be divided into a <strong>SIMT front-end</strong> and a <strong>SIMD back-end</strong>. The pipeline consists of three scheduling “loops” acting together in a single pipeline: 1. an <strong>instruction fetch</strong> loop, 2. an <strong>instruction issue</strong> loop, and 3. a <strong>register access</strong> scheduling loop.</p>
<h3 id="1-Instruction-Fetch-Loop"><a href="#1-Instruction-Fetch-Loop" class="headerlink" title="1. Instruction Fetch Loop"></a>1. Instruction Fetch Loop</h3><p>To increase efficiency, threads are organized into groups called “warps” by NVIDIA and “wavefronts” by AMD. Thus, <strong>the unit of scheduling is a warp</strong>.</p>
<p>After fetching an instruction, the instruction is decoded and source operand registers are fetched from the register file. In parallel with fetching source operands from the register file, the SIMT execution mask values are determined.</p>
<p>As in modern CPU designs, the <strong>function units</strong> are typically heterogeneous meaning a given function unit supports only a subset ofinstructions. For example, NVIDIA GPUs contain a <em>specialfunction unit</em> (SFU), <em>load/store unit, floating-point function unit, integer function unit</em>, and, as of Volta, a <em>Tensor Core</em>.</p>
<p>SIMT execution model presents the programmer with the abstraction that individual threads execute completely independently. However, in current GPUs it is achieved via a combination of traditional <em>predication</em> along with a stack of predicate masks that we shall refer to as the <strong>SIMT stack</strong>.</p>
<p>The SIMT stack helps efficiently handle two key issues: nested control flow and skipped computation.</p>
<p>A <strong>reconvergence point</strong> is a location in the program where threads that diverge can be forced to continue executing in lock-step.</p>
<p>The stack-based implementation of SIMT can lead to a deadlock condition called “<strong>SIMT deadlock</strong>”.</p>
<p>NVIDIA ‘s new thread divergence management approach is called <em>Independent Thread Scheduling</em> and it can fix SIMT deadlock by introducing a new structure <em>covergence barrier</em>. In contrast to a stack-based SIMT implementation, with convergence barrier implementation the scheduler is free to switch between groups of diverged threads.</p>
<img src="/2022/01/15/R16/5.png" class="">

<p>Each core in a GPU hosts contains many warps. To enable a different warp to issue an instruction <strong>each cycle</strong>, it is necessary that each thread have its own registers (avioding <em>context switch</em> occured in traditional CPU). The tradeoff is: <strong>increasing the number of warps per core increases the fraction of chip area devoted to register file storage relative to the fraction dedicated to execution units</strong>. (For a fixed chip area increasing warps per core will decrease the total nymber of cores per chip.)</p>
<h3 id="2-Instruction-Issue-Loop"><a href="#2-Instruction-Issue-Loop" class="headerlink" title="2. Instruction Issue Loop"></a>2. Instruction Issue Loop</h3><p>To help reduce the number of warps that each core must support to hide long execution latencies, it is helpful to be able to <strong>issue a subsequent instruction from a warp while earlier instructions have not yet completed</strong>. Thus, supporting many warps per core can help hide long execution latencies.</p>
<p>GPUs implement an instruction buffer were instructions are placed after cache access. A separate scheduler is used to decide which of several instructions in the instruction buffer should be issued next to the rest of the pipeline.</p>
<p>There are two traditional approaches to detecting <strong>dependencies</strong> between instructions found in traditional CPU architecture: <em>scoreboard</em> and <em>reservation stations</em>. (bty, scoreboards support out-of-order execution.)</p>
<p>Given it is the simplest design and therefore will consume the least amount of area and energy, GPUs implement <strong>in-order scoreboards</strong>.</p>
<h3 id="3-Register-Access-Scheduling-Loop"><a href="#3-Register-Access-Scheduling-Loop" class="headerlink" title="3. Register Access Scheduling Loop"></a>3. Register Access Scheduling Loop</h3><p>To hide long memory latencies it is necessary to support many warps per core and to support cycle by cycle switching between warps it is necessary to have a large register file that contains separate physical registers for every warp that is executing. One way to reduce the area of the register file is to simulate the large number of ports using multiple <strong>banks</strong> of single-ported memories. In some GPUs, this kind of structure is known as the <strong>operand collector</strong>.</p>
<p>Operand collector microarchitecture</p>
<img src="/2022/01/15/R16/6.png" class="">

<p>Each collector unit contains buffering space for all source operands required to execute an instruction. The operand collector uses scheduling to tolerate <strong>bank conflicts</strong> when they occur.</p>
<p>Some CPUs speculative wake up instructions depending upon a load so as to improve single threaded performance. In contrast, GPUs avoid <strong>speculation</strong> as it tends to waste energy and reduce throughput. Instead, <strong>instruction replay</strong> is used in GPUs to avoid clogging the pipeline and the circuit area and/or timing overheads resulting from stalling.</p>
<h2 id="Memory-System"><a href="#Memory-System" class="headerlink" title="Memory System"></a>Memory System</h2><p><strong>Pipeline hazards can be handled by replaying instructions</strong>.</p>
<p><strong>Shared memory</strong> in CUDA (or local memory in OpenCL) and <strong>L1 cache</strong> is a same thing except shared memory is controlled by programmer whereas L1 cache is controlled by hardware. They are implemented as a static random access memory (SRAM).</p>
<p>Below is an <em>unified L1 data cache and shared memory</em> architecture. #5 is an SRAM data array which can be configured partly for <strong>direct mapped access</strong> for shared memory and partly as a set associative cache.</p>
<img src="/2022/01/15/R16/7.png" class="">

<p>There are three kinds of opertions to this architecture: shared memory access operations, cache read operations, cache write operations.</p>
<p>The SIMT cores connect to the memory partition units via an on-chip interconnection network. For NVIDIA it’s called <strong>crossbar</strong>, and for AMD it’s called <strong>ring network</strong>.</p>
<p>Refer to the abstraction architecture shown above in section 1, <strong>Memory Partition Unit</strong> is under interconnect network. Each memory partition unit contains a portion of the <strong>second-level (L2) cache</strong> along with a one or more memory access schedulers also called a “frame buffer,” or <strong>FB</strong>, and a raster operation (<strong>ROP</strong>) unit. The L2 cache contains both graphics and compute data.</p>
<img src="/2022/01/15/R16/8.png" class="">

<p>The ROP unit includes function units for executing atomic and reduction operations.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2022/01/06/R7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/06/R7/" class="post-title-link" itemprop="url">007 "A Formal Analysis of the NVIDIA PTX Memory Consistency Model"</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-06 20:30:12" itemprop="dateCreated datePublished" datetime="2022-01-06T20:30:12-05:00">2022-01-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-13 17:20:48" itemprop="dateModified" datetime="2022-02-13T17:20:48-05:00">2022-02-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reviews/" itemprop="url" rel="index"><span itemprop="name">Reviews</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em><strong>Keywards</strong></em> Memory Consistency Models, GPUs, Theorem Proving, Model Finding, SAT Solving</p>
<blockquote>
<p>Author: Daniel Lustig, Sameer Sahasrabuddhe, Olivier Giroux</p>
</blockquote>
<hr>
<h2 id="What-is-the-Problem"><a href="#What-is-the-Problem" class="headerlink" title="What is the Problem?"></a>What is the Problem?</h2><p>There isn’t a well-accepted memory consistency model for NVIDIA GPU.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>This paper presents the first formal analysis of the offical memory consistency model for the NVIDIA PTX virtual ISA. However, unlike some competing GPU memory models, PTX does not require data race freedom, and this results in PTX using a fundamentally different (and more complicated) set of rules in its memory model.</p>
<h2 id="Contribution-Key-Results"><a href="#Contribution-Key-Results" class="headerlink" title="Contribution / Key Results"></a>Contribution / Key Results</h2><p>The major contribution of this paper is a formalization and rigorous analysis of PTX memory model.</p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ul>
<li>NVIDIA provides the abstraction <strong>Independent Therad Scheduling</strong> that each thread is treated as an independent unit of execution.</li>
<li>They build this MCM based on CUDA 9.0 with PTX 6.0.</li>
<li>One important requirement for any memory model is the ability to reliably serve as a target for higher-level memory models (e.g., from CUDA or OpenCL) that will be compiled onto it.</li>
</ul>
<h2 id="Key-Insights"><a href="#Key-Insights" class="headerlink" title="Key Insights"></a>Key Insights</h2><p>Steps of their formal analysis of the PTX memory model:</p>
<ol>
<li>First, adapt the English language specification from the public PTX documentation into a formal aximatic model.</li>
<li>Second, derive an up-to-date presentation of an OpenCL-like scoped C++ model and develop a mapping from the synchronization primitives of that scoped C++ model onto PTX.</li>
<li>Third, use the Alloy relational modeling tool to empirically test the correctness of the mapping.</li>
<li>Finally, compile the model and the mapping into Coq and build a full machine-checked proof that the mapping is sound for programs of any size.</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2022/01/03/R4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/03/R4/" class="post-title-link" itemprop="url">004 "big.LITTLE Technology - The future of Mobile"</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-03 22:13:23" itemprop="dateCreated datePublished" datetime="2022-01-03T22:13:23-05:00">2022-01-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-13 17:20:52" itemprop="dateModified" datetime="2022-02-13T17:20:52-05:00">2022-02-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reviews/" itemprop="url" rel="index"><span itemprop="name">Reviews</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em><strong>Keywards</strong></em> big.LITTLE Technology, white paper</p>
<blockquote>
<p>Author: ARM</p>
</blockquote>
<hr>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>We are in a situation where smartphones require higher performance, but the same power consumption. ARM Big.LITTLE technology is a heterogeneous processing architecture which uses two types of processor.</p>
<ul>
<li><strong>LITTLE</strong> processors are designed for maximum power efficiency.</li>
<li><strong>big</strong> processors are designed to provide maximum compute performance.</li>
</ul>
<p>Both types of processor are <strong>coherent</strong> and <strong>share the same ISA</strong>.</p>
<h2 id="Same-architecture-but-different-micro-architectures"><a href="#Same-architecture-but-different-micro-architectures" class="headerlink" title="Same architecture but different micro-architectures"></a>Same architecture but different micro-architectures</h2><p>Here describes the pipeline designs for the Cortex-A15 and Cortex-A7 cores.</p>
<img src="/2022/01/03/R4/1.png" class="">

<ul>
<li>The Cortex-A15 core is designed to achieve high performance by running more instructions in parallel on a bigger and more complex pipeline.</li>
<li>The Cortex-A7 core’s pipeline is relatively simple and is designed to be extremely power efficient.</li>
</ul>
<img src="/2022/01/03/R4/2.png" class="">

<p>The basic idea of big.LITTLE technology is to dynamically allocate tasks to the right processor according to their instantaneous performance requirement. Performance hungry tasks can be <strong>migrated</strong> to the Cortex-A15 core cluster. When the performance requirement reduces, tasks can be <strong>re-allocated</strong> to the Cortex-A7 cluster and Cortex-A15 cores may be then turned off.</p>
<h2 id="Cache-Coherency-Interface-and-big-LITTLE-Technology"><a href="#Cache-Coherency-Interface-and-big-LITTLE-Technology" class="headerlink" title="Cache Coherency Interface and big.LITTLE Technology"></a>Cache Coherency Interface and big.LITTLE Technology</h2><p>The key ingredient that makes big.LITTLE technology possible is <strong>coherency</strong>. big.LITTLE software models require <strong>transparent</strong> and <strong>performant</strong> transfer of data between big and LITTLE processors.</p>
<img src="/2022/01/03/R4/3.png" class="">

<p><strong>Cache Coherent Interconnect</strong> (CCI) enable the seamless data transfer between clusters. (In this figure, the CCI-400). Besides, ARM also use AMBA AXI Coherency Extensions (ACE) protocol to provides for coherent data transfer at the <strong>bus level</strong>. The CCI-400 and the ACE protocol enable full coherency between the Cortex-A15 and Cortex-A7 clusters, allowing data sharing to take place without external memory transactions.</p>
<h2 id="Software-execution-models-for-big-LITTLE"><a href="#Software-execution-models-for-big-LITTLE" class="headerlink" title="Software execution models for big.LITTLE"></a>Software execution models for big.LITTLE</h2><p>Two major software models:</p>
<ul>
<li><strong>CPU Migration</strong>: Each big core is paried with a LITTLE core. Only one core in each pair is active at any one time, with the inactive core being powered down. The choice made between big core and LITTLE core is driven by DVFS (dynamic voltage/frequency scaling).</li>
<li><strong>Global Task Scheduling</strong> (GTS): Using stastistical data and other heuristics, the scheduler tracks the performance requirement for each individual thread, and uses that information to decide which type of processor to use for each thread.</li>
</ul>
<img src="/2022/01/03/R4/4.png" class="">

<p>Advantages GTS model have than CPU Migration model</p>
<ol>
<li>The system can have different numbers of big and LITTLE cores.</li>
<li>Any number of cores may be active at any one time. When peak performance is required the system can deploy all cores. With CPU Migration only half of the cores may be active at any one time.</li>
<li>It is possible to isolate the big cluster for the exclusive use of intensive threads, whilst light threads run on the LITTLE cluster . With CPU Migration, all the threads in a processor transfer together. This allows heavy compute tasks to complete faster, as there are no additional background threads.</li>
<li>It is possible to target interrupts individually to big or LITTLE cores. The CPU Migration model assumes all context, including interrupt targetting, migrates between big and LITTLE processors.</li>
</ol>
<p>Therefore, ARM use GTS in their implementation and it’s known as <strong>big.LITTLE MP</strong>.</p>
<h2 id="Global-Task-Scheduling"><a href="#Global-Task-Scheduling" class="headerlink" title="Global Task Scheduling"></a>Global Task Scheduling</h2><p>The ARM big.LITTLE MP solution uses two configurable thresholds and the tracked load metric to decide whether and when to allocate a thread to a big or LITTLE core.</p>
<ul>
<li><strong>up migration threshold</strong>: When on a LITTE core the tracked load average of a thread exceeds this threshold, the thread is eligible for migration to a big core.</li>
<li><strong>down migration threshold</strong>: When on a big core the tracked load average of a thread below this threshold, the thread is eligible for migration to a LITTLE core.</li>
</ul>
<p>Within each clusters, standard Linux scheduler load balancing also applies, which try to keep the load balanced among all the cores in one cluster.</p>
<p>The ARM big.LITTLE MP solution uses a number of software thread affinity management techniques to determine when to migrate a task between big and LITTLE processors:</p>
<ul>
<li>fork migration</li>
<li>wake migration</li>
<li>forced migration</li>
<li>idle-pull migration</li>
<li>offload migration</li>
</ul>
<h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p>big.LITTLE MP Power Savings compared to a Cortex-A15 processor-only based system</p>
<img src="/2022/01/03/R4/5.png" class="">

<p>big.LITTLE MP Benchmark Improvements</p>
<img src="/2022/01/03/R4/6.png" class="">

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2022/01/02/R3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/02/R3/" class="post-title-link" itemprop="url">003 "Viewpoint Accelerator-Level Parallelism"</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-02 20:04:12" itemprop="dateCreated datePublished" datetime="2022-01-02T20:04:12-05:00">2022-01-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-16 17:21:16" itemprop="dateModified" datetime="2022-02-16T17:21:16-05:00">2022-02-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reviews/" itemprop="url" rel="index"><span itemprop="name">Reviews</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em><strong>Keywards</strong></em> review, accelerator-level parallelism</p>
<blockquote>
<p>Author: Mark D. Hill and Vijay Janapa Reddi</p>
</blockquote>
<hr>
<h2 id="What-is-the-Problem"><a href="#What-is-the-Problem" class="headerlink" title="What is the Problem?"></a>What is the Problem?</h2><p>Recent work on accelerators has focused on CPUs using a single accelerator in a specific application domains. The authors point out that many future computing systems will obtain greater efficiency by employing multiple different task-specific accelerators.</p>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p><strong>Accelerator</strong>: <em>hardware components that execute a targeted computation class faster and usually with much less energy</em>.</p>
<p>An accelerator’s flexibility can vary from high (GP-GPU) to low (fixed-function block).</p>
<p>Below is a snapshot of parallelism over the years:</p>
<img src="/2022/01/02/R3/1.png" class="">

<h2 id="Key-Insights"><a href="#Key-Insights" class="headerlink" title="Key Insights"></a>Key Insights</h2><ul>
<li>Definition of a Accelerator Level Parallelism (ALP): <em>the parallelism of workload components concurrently executing on multiple accelerators</em>.</li>
<li>ALP opens up many degrees of freedom for novel hardware and software design and optimization.</li>
<li>Mobile SoCs as harbingers of multiple accelerators using ALP.</li>
</ul>
<h2 id="Challenges-and-Future"><a href="#Challenges-and-Future" class="headerlink" title="Challenges and Future"></a>Challenges and Future</h2><p>A key challenge is developing abstractions and implementations to enable programmers to target the whole SoC and implementers to holistically design its software and hardware. (like SIMT model)</p>
<ul>
<li>Today’s SoCs only exploit ALP in limited niches with each accelerator acting seperately with its own programming model, and often its own (domian-specific) language, runtime, software development kit (SDK), and dtiver interface.</li>
<li>ALP exploitation will likely require software-hardware co-design due to the heterogeneous nature of accelerators and ALP. A collection of locally optimal accelerators is unlikely to be globally optimal.</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Employing mutiple accelerators with ALP has much promise for enhanting future computing efficiency. The authors have identified <em>what</em> the opportunity is, but leave to readers <em>how</em> best to solve it.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2022/01/01/R2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/01/R2/" class="post-title-link" itemprop="url">002 "Near-memory computing:Past, present, and future"</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-01-01 21:04:13" itemprop="dateCreated datePublished" datetime="2022-01-01T21:04:13-05:00">2022-01-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-16 17:14:43" itemprop="dateModified" datetime="2022-02-16T17:14:43-05:00">2022-02-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reviews/" itemprop="url" rel="index"><span itemprop="name">Reviews</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em><strong>Keywards</strong></em> Near-memory computing, Data-centric computing, Modeling, Computer architecture, Application characterization,Survey</p>
<blockquote>
<p>Author: Gagandeep Singh, Lorenzo Chelini, Stefano Corda, Ahsan Javed Awan, Sander Stuijk, Roel Jordans, Henk Corporaal, Albert-Jan Boonstra</p>
</blockquote>
<hr>
<h2 id="What-is-the-Problem"><a href="#What-is-the-Problem" class="headerlink" title="What is the Problem?"></a>What is the Problem?</h2><p>Today’s memory hierarchy usually consists of multiple levels of cache. However, in data-intensive applications,<br> frequent data movement between the memory subsystem and the processor can be expensive, leading to a severe impact on performance and energy efficiency. <strong>Near-memory computing</strong> (NMC) aims at processing close to where the data resides.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>The convential approach of moving data to the CPU for computation has become a significant performance bottleneck for emerging scale-out data-intensive applications due to their limited data reuse. At the same time, the advancement in 3D integration technologies has made <strong>near-memory computing</strong> (NMC) more viable. This paper aims to analyze and organize the extensive body of literature related to the novel area of near-memory computing.</p>
<h2 id="Contribution-Key-Results"><a href="#Contribution-Key-Results" class="headerlink" title="Contribution / Key Results"></a>Contribution / Key Results</h2><ul>
<li>They analyze and organize the body of literature on near-memory computing under various dimensions.</li>
<li>They provide guidelines for design space exploration.</li>
<li>They present our near-memory system outlining application characterization and compiler framework. Also, we include an analytic model to illustrate the potential of near-memory computing for memory intensive application with a comparison to the current CPU-centric approach.</li>
<li>They outline the directions for future research and highlight current challenges.</li>
</ul>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Classification of computing systems based on working set location. NMC bring processing core to the place where data resides. Computation-in-memory further reduces data movement by using memories with compute capability (e.g. memristors, phase change memory).</p>
<img src="/2022/01/01/R2/1.png" class="">

<h2 id="Key-Insights"><a href="#Key-Insights" class="headerlink" title="Key Insights"></a>Key Insights</h2><p>The resurgence of NMC is largely attributed to following three reasons:</p>
<ol>
<li>technological advancement in 3D and 2.5D stacking that blends logic and memory in the same package.</li>
<li>NMC can sidestep the performance and energy bottlenecks due to data movement by circumventing memory-package pin-count limitations.</li>
<li>Data-intensive applications call for newer architectures.</li>
</ol>
<h2 id="Challenges-of-NMC"><a href="#Challenges-of-NMC" class="headerlink" title="Challenges of NMC"></a>Challenges of NMC</h2><ol>
<li>Virtual memory support</li>
<li>Cache coherence</li>
<li>Programming model</li>
<li>Data mapping</li>
</ol>
<h2 id="Open-Issues-and-Future-Directions"><a href="#Open-Issues-and-Future-Directions" class="headerlink" title="Open Issues and Future Directions"></a>Open Issues and Future Directions</h2><ul>
<li>It is unclear which emerging memory technology best supports near-memory architectures.</li>
<li>3D stacking also needs unique power and thermal solutions, as traditional heat sink technology would not suffice if we want to add more computation close to the memory. Most of the proposed architectures do not take into account the strict power budget of 3D stacked memories, which limits the architecture’s practicality.</li>
<li>DRAM and NVM have different memory attributes.</li>
<li>Most of the evaluated architectures focus on the compute aspect. Lack of coherency and virtual support makes programming difficult and obstructs the adoption of this paradigm.</li>
<li>More quantitative exploration is required for interconnect networks between the near-memory compute units and also between the host and near-memory compute system.</li>
<li>At the application level, algorithms need to provide code and data co-location for energy efficient processing.</li>
<li>The field requires a generic set of open-source tools and techniques for these novel systems, as often researchers have to spend a significant amount of time and effort in building the needed simulation environment.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Godway"
      src="/images/avatar.ico">
  <p class="site-author-name" itemprop="name">Godway</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">66</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="/475865836@qq.com" title="E-Mail → 475865836@qq.com"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://randool.cn/" title="https:&#x2F;&#x2F;randool.cn" rel="noopener" target="_blank">Randool</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://me.csdn.net/Smile_coderrr" title="https:&#x2F;&#x2F;me.csdn.net&#x2F;Smile_coderrr" rel="noopener" target="_blank">Smile_coderrr</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Godway</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
