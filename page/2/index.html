<!DOCTYPE html>
<html lang="">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_16x16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wshenyi.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":300,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Godway&#39;s Notebook">
<meta property="og:url" content="https://wshenyi.github.io/page/2/index.html">
<meta property="og:site_name" content="Godway&#39;s Notebook">
<meta property="og:locale">
<meta property="article:author" content="Godway">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://wshenyi.github.io/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'default'
  };
</script>

  <title>Godway's Notebook</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Godway's Notebook</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">wsy</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/WSHENYI/WSHENYI.github.io" class="github-corner" title="Star me on GitHub" aria-label="Star me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2020/02/29/RL3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/29/RL3/" class="post-title-link" itemprop="url">RL3：Value Functions & Bellman Equations</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-29 11:14:01" itemprop="dateCreated datePublished" datetime="2020-02-29T11:14:01-05:00">2020-02-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-17 11:19:49" itemprop="dateModified" datetime="2020-03-17T11:19:49-04:00">2020-03-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<blockquote>
<p>Refer from <em>Reinforcement Learning An Introduction (2nd)</em> and <em>Reinforcement Learning specialization on Coursera by University of Alberta &amp; Alberta Machine Intelligence Institute</em></p>
</blockquote>
<h2 id="policies-and-value-functions">Policies and Value Functions</h2>
<h3 id="policy">Policy</h3>
<p>Formally, a <strong>policy</strong> is a mapping from <strong>states</strong> to probabilities of selecting each possible <strong>action</strong>.</p>
<ul>
<li><strong>deterministic policy</strong>, denote <span class="math inline">\(\pi(s)=a\)</span>: one state correspond to one action</li>
<li><strong>stochastic policy</strong>, <span class="math inline">\(\pi(a|s)\)</span>: a probability distribution over all visable actions</li>
</ul>


<p>It's important that <strong>policies depend only on the current state</strong>, not on other things like time or previous states.</p>
<h3 id="state-value-function">State-value Function</h3>
<p>Almost all reinforcement learning algorithms involve <strong>estimating value functions</strong> -- functions of states (or of state–action pairs) that estimate <strong>how good</strong> it is for the agent to be in a given state (or <strong>how good</strong> it is to perform a given action in a given state), which means value function predicts reward into future.</p>
<p>The value function of a state <span class="math inline">\(s\)</span> under a policy <span class="math inline">\(\pi\)</span>, denoted <span class="math inline">\(v_\pi(s)\)</span>, is <strong>the expected return when starting in <span class="math inline">\(s\)</span> and following <span class="math inline">\(\pi\)</span> thereafter</strong>. For MDPs, we can define <span class="math inline">\(v_\pi\)</span> formally by</p>
<p><span class="math display">\[v_\pi(s)\doteq \mathbb{E}_\pi[G_t|S_t=s]=\mathbb{E}_\pi[\sum_{k=0}^\infty \gamma^kR_{t+k+1}|S_t=s],\ for\ all\ s\in \mathbf{S}\]</span></p>
<p>We call the function <span class="math inline">\(v_\pi\)</span> the <strong>state-value function for policy <span class="math inline">\(\pi\)</span></strong></p>
<h3 id="action-value-function">Action-value Function</h3>
<p>Similarly, we define the value of taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> under a policy <span class="math inline">\(\pi\)</span>, denoted <span class="math inline">\(q_\pi(s,a)\)</span>, as <strong>the expected return starting from <span class="math inline">\(s\)</span>, taking the action <span class="math inline">\(a\)</span>, and thereafter following policy <span class="math inline">\(\pi\)</span></strong>:</p>
<p><span class="math display">\[q_\pi(s,a)\doteq \mathbb{E}_\pi[G_t|S_t=s,A_t=a]=\mathbb{E}_\pi[\sum_{k=0}^\infty \gamma^kR_{t+k+1}|S_t=s,A_t=a]\]</span></p>
<p>We call the function <span class="math inline">\(q_\pi\)</span> the **action-value function for policy *,</p>
<hr />
<h2 id="bellman-equations">Bellman Equations</h2>
<h3 id="transform-between-v_pi-and-q_pi">Transform Between <span class="math inline">\(v_\pi\)</span> And <span class="math inline">\(q_\pi\)</span></h3>
<p>Through the given diagram, we can derivate an equation for <span class="math inline">\(v_\pi\)</span> in terms of <span class="math inline">\(q_\pi\)</span> and <span class="math inline">\(\pi\)</span>.</p>

<p><span class="math display">\[
\begin{eqnarray*}
v_\pi(s)
&amp;=&amp;\mathbb{E}_\pi[G_t|S_t=s]\\
&amp;=&amp;\sum_a\pi(a|s)\mathbb{E}[G_t|S_t=s,A_t=a]\\
&amp;=&amp;\sum_a\pi(a|s)q(s,a)\tag{3.1}
\end{eqnarray*}
\]</span> Recall that <span class="math inline">\(G_t=R_{t+1}+\gamma G_{t+1}\)</span>. With given diagram below, we can derivate an equation for <span class="math inline">\(q_\pi\)</span> in terms of <span class="math inline">\(v_\pi\)</span> and <span class="math inline">\(p(s&#39;,r|s,a)\)</span>. (use markov property to simplify original equation)</p>

<p><span class="math display">\[
\begin{eqnarray*}
q_\pi(s,a)
&amp;=&amp;\mathbb{E}_\pi[G_t|S_t=s,A_t=a]\\
&amp;=&amp;\mathbb{E}_\pi[R_{t+1}+\gamma G_{t+1}]\\
&amp;=&amp;\sum_{s&#39;,r}p(s&#39;,r|s,a)(r+\gamma \mathbb{E}_\pi[G_{t+1}|S_{t+1}=s&#39;])\\
&amp;=&amp;\sum_{s&#39;,r}p(s&#39;,r|s,a)(r+\gamma v_\pi(s&#39;))\tag{3.2}
\end{eqnarray*}
\]</span></p>
<h3 id="bellman-equations-for-v_pi">Bellman Equations for <span class="math inline">\(v_\pi\)</span></h3>
<p><strong>Bellamn equation allows us to relate the value of the current state to the value of future states without waiting to observe all the future rewards.</strong><br />
With <span class="math inline">\((3.1),(3.2)\)</span>, we can easiy get bellamn equation for <span class="math inline">\(v_\pi\)</span></p>
<p><span class="math display">\[
\begin{eqnarray*}
v_\pi(s)
&amp;=&amp;\sum_a\pi(a|s)q(s,a)\\
&amp;=&amp;\sum_a\pi(a|s)\sum_{s&#39;,r}p(s&#39;,r|s,a)(r+\gamma v_\pi(s&#39;))\tag{3.3}
\end{eqnarray*}
\]</span></p>
<p><span class="math inline">\(s&#39;\)</span> here stands for next state. <strong>Bellamn equation expresses a relationship between the value of a state and the values of its successor states (next state).</strong> You can get a more intiutive understanding about how this equation forms though its backup diagram below: <strong>we first choose actions through policy and then transites to a successor state with corresponding reward under environment dynamics</strong>. The open circles represent state nodes and solid circles represent action nodes.</p>

<h3 id="bellman-equations-for-q_pi">Bellman Equations For <span class="math inline">\(q_\pi\)</span></h3>
<p>With <span class="math inline">\((3.1),(3.2)\)</span>, we can easiy get bellamn equation for <span class="math inline">\(q_\pi\)</span></p>
<p><span class="math display">\[
\begin{eqnarray*}
q_\pi(s,a)
&amp;=&amp;\sum_{s&#39;,r}p(s&#39;,r|s,a)(r+\gamma v_\pi(s&#39;))\\
&amp;=&amp;\sum_{s&#39;,r}p(s&#39;,r|s,a)(r+\gamma\sum_{a&#39;} q_\pi(s&#39;,a&#39;))\tag{3.4}
\end{eqnarray*}
\]</span></p>
<p><span class="math inline">\(a&#39;\)</span> here stands for actions under possible state <span class="math inline">\(s&#39;\)</span>. Similarly, <strong>this equation express a relationship betweeen the value of an action and the values of actions under possible successor state <span class="math inline">\(s&#39;\)</span></strong></p>

<h3 id="an-example-of-value-function-grideworld">An Example of Value Function: Grideworld</h3>
<ul>
<li><strong>State</strong>: each grid (totally 25 states)</li>
<li><strong>Action</strong>: north, south, east, and west</li>
<li><strong>Reward</strong>: actions that would take the agent off the grid leave its location unchanged, but also result in a reward of -1. Other actions results in a reward 0. Every time the agent moves any directions at position A would be transited to A' with reward +10. This is the same to B and B' except for reward +5.</li>
<li><strong>Policy</strong>: random policy, each direction has equal probability, 25%.</li>
</ul>
<p><span class="math inline">\(\gamma=0.9\)</span></p>

<p>Now, just for curious, we can verify the <span class="math inline">\(v_\pi(s)\)</span> bellman equation in this example. Let's say we choose center state.</p>
<p><span class="math display">\[(0.7\times0.9+0)\times0.25+(2.3\times0.9+0)\times0.25+(0.4\times0.9+0)\times0.25\\+(-0.4\times0.9+0)\times0.25=0.69\approx0.7\]</span></p>
<h3 id="why-bellman-equations">Why Bellman Equations?</h3>
<p>We use bellman equations to calculate <span class="math inline">\(v_\pi(s)\)</span> for all <span class="math inline">\(s\in \mathbf{S}\)</span> by solving the system of linear equations.</p>
<p>Still the previous example, gride world. It has 25 states and we get 25 unknown parameters. For each state we can get a bellman equation, therefore we get 25 equations. Through 25 equations, we sovle them to obtain 25 state values.</p>
<p>But bellman equation is not pragmatic for many reasons. One of them is that sometimes the number of states is too large and it's impossible to calculate results in a short time. It can <strong>only solve small MDPs</strong>.</p>
<hr />
<h2 id="optimality-optimal-policies-value-functions">Optimality (Optimal Policies &amp; Value Functions)</h2>
<h3 id="optimal-policy">Optimal Policy</h3>
<p>A policy <span class="math inline">\(\pi\)</span> is defined to be better than or equal to a policy <span class="math inline">\(\pi&#39;\)</span> if its expected return is greater than or equal to that of <span class="math inline">\(\pi&#39;\)</span> for all states. In other words,</p>
<p><span class="math display">\[\pi&gt;\pi&#39; \ \ iff\ v_\pi(s)\ge v_{\pi&#39;}(s)\ for\ all\ s \in \mathbf{S}\]</span></p>

<p><strong>There is always at least one deterministic policy that is better than or equal to all other policies. This is an optimal policy.</strong> Although <strong>there may be more than one</strong>, we denote all the optimal policies by <span class="math inline">\(\pi_*\)</span>. They share the same state-value function, called the <strong>optimal state-value function</strong>, denoted <span class="math inline">\(v_*\)</span>, and the same <strong>optimal action-value function</strong>, denoted <span class="math inline">\(q_*\)</span></p>
<p><span class="math display">\[v_*(s)\doteq \mathop{max}_\pi\ v_\pi(s)\\
q_*(s,a)\doteq \mathop{max}_\pi\ q_\pi(s,a)\\
for\ all\ s\in \mathbf{S},a\in \mathbf{A}\]</span></p>
<h3 id="optimal-value-functions">Optimal Value Functions</h3>
<p>Because the optimal policy <span class="math inline">\(\pi_*\)</span> is a <strong>deterministic policy</strong>, the optimal action <span class="math inline">\(a_*\)</span> with <span class="math inline">\(\pi(a_*|s)\)</span> equals to 1 and all other actions' probability equal to 0. We can re-write <span class="math inline">\(v_*\)</span> with bellman equation as below, called <strong>bellman optimality equation for <span class="math inline">\(v_*\)</span></strong>.</p>
<p><span class="math display">\[
\begin{eqnarray*}
v_*(s)
&amp;=&amp;\mathop{max}_\pi\ v_\pi(s)\\
&amp;=&amp;\mathop{max}_a \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a]\\
&amp;=&amp;\mathop{max}_a\ \sum_{s&#39;,r}p(s&#39;,r|s,a)(r+\gamma v_*(s&#39;))\tag{3.5}
\end{eqnarray*}
\]</span></p>
<p>Similarly, <strong>bellman optimality equation for <span class="math inline">\(q_*\)</span></strong> is</p>
<p><span class="math display">\[
\begin{eqnarray*}
q_*(s,a)
&amp;=&amp;\mathop{max}_\pi\ q_\pi(s,a)\\
&amp;=&amp;\mathbb{E}[R_{t+1}+\gamma \mathop{max}_{a&#39;}\ q_*(S_{t+1},a&#39;)|S_t=s,A_t=a]\\
&amp;=&amp;\sum_{s&#39;,r}p(s&#39;,r|s,a)(r+\gamma \mathop{max}_{a&#39;}\ q_*(s&#39;,a&#39;))\tag{3.6}
\end{eqnarray*}
\]</span></p>

<p>The <strong>bellman optimality equations</strong> relate the value of a state or state-action pair, to it's possible successors under <strong>any optimal policy</strong>.</p>
<p>Unfortunately, the <span class="math inline">\(max\)</span> funtion here is <strong>not linear</strong>, we <strong>cannot through linear system solver</strong> to directly solve bellman optimality equation to obtain all <span class="math inline">\(v_*\)</span> in MDPs. Although, in principle, one can use a variety of methods for solving systems of nonlinear equations, it is not recommended due to its extreme computational cost.</p>
<h3 id="get-optimal-policy">Get Optimal Policy</h3>
<p>Once one has <span class="math inline">\(v_*\)</span>, it is relatively easy to determine an optimal policy. Because, for each state <span class="math inline">\(s\)</span>, there will be one or more actions at which the maximum is obtained in the Bellman optimality equation. That is to say <strong>any policy that is greedy with respect to the optimal evaluation function <span class="math inline">\(v_*\)</span> is an optimal policy</strong>.</p>

<p>Having <span class="math inline">\(q_*\)</span> makes choosing optimal actions even easier. The action-value function effectively caches the results of all one-step-ahead searches.</p>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2020/02/28/RL2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/28/RL2/" class="post-title-link" itemprop="url">RL2：Markov Decision Processes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-28 17:21:31" itemprop="dateCreated datePublished" datetime="2020-02-28T17:21:31-05:00">2020-02-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-03 03:34:00" itemprop="dateModified" datetime="2020-04-03T03:34:00-04:00">2020-04-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<blockquote>
<p>Refer from <em>Reinforcement Learning An Introduction (2nd)</em> and <em>Reinforcement Learning specialization on Coursera by University of Alberta &amp; Alberta Machine Intelligence Institute</em></p>
</blockquote>
<h2 id="introduction-to-markov-decision-processes">Introduction to Markov Decision Processes</h2>
<h3 id="definition-of-mdp">Definition of MDP</h3>
<p>MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal. Here are the key element in discribing a MDP</p>
<ul>
<li><strong>agent</strong>: the learner and the decision maker</li>
<li><strong>environment</strong>: the thing a agent interacts with, comprising everything outside the agent</li>
<li><strong>action</strong>: selected by the agent to interact with the environment</li>
<li><strong>reward</strong>: special numerical values that the agent seeks to maximize over time through its choice of actions.</li>
</ul>
<p>the whole interacting process is precisely simplified as follow<br />
<br />
More specifically, the agent and the environment interact at each of a sequence of discrete time steps, <span class="math inline">\(t=0,1,2,3,...\)</span> At each time step <span class="math inline">\(t\)</span>, the agent recives some representation of the environment's <strong>state</strong>, <span class="math inline">\(S_t \in \mathbf{S}\)</span>, and on that basis selects an <strong>action</strong>, <span class="math inline">\(A_t \in \mathbf{A}(s)\)</span>.</p>
<p>One time step later, in part as a consequence of its action, the agent receives a numerical reward, <span class="math inline">\(R_{t+1} \in \mathbf{R}\subset \mathbb{R}\)</span>, and finds itself in a new state, <span class="math inline">\(S_{t+1}\)</span>. The MDP and agent together thereby give rise to a sequence or trajectory that begins like this:<br />
<br />
In a finite MDP, the sets of states, actions, and rewards (<span class="math inline">\(\mathbf{S}\)</span>, <span class="math inline">\(\mathbf{A}\)</span>, and <span class="math inline">\(\mathbf{R}\)</span>) all have a finite number of elements. In this case, the random variables <span class="math inline">\(R_t\)</span> and <span class="math inline">\(S_t\)</span> have well defined discrete probability distributions dependent only on the preceding state and action. That is, for particular values of these random variables, <span class="math inline">\(s&#39; \in \mathbf{S}\)</span> and <span class="math inline">\(r \in \mathbf{R}\)</span>, there is a probability of those values occurring at time t, given particular values of the preceding state and action: <span class="math display">\[p(s&#39;,r|s,a)\doteq P(S_t=s&#39;, R_t=r\ | \ S_{t-1}=s, A_{t-1}=a)\tag{2.1}\]</span> for all <span class="math inline">\(s&#39;,s \in \mathbf{S}\)</span>, <span class="math inline">\(r \in \mathbf{R}\)</span>, and <span class="math inline">\(a \in \mathbf{A}(s)\)</span>. The function <span class="math inline">\(p\)</span> defines the <strong>dynamics of the MDP</strong>. <span class="math display">\[p:\ \mathbf{S}\times \mathbf{R}\times \mathbf{S}\times \mathbf{A}\rightarrow [0,1]\]</span> With <span class="math inline">\(p(s&#39;,r|s,a)\)</span>, note that <strong>future state and reward only depends on the current state and action</strong>. This is called the <strong>Markov property</strong>. It means that the present state is sufficient and remembering earlier states would not improve predictions about the future.</p>
<p>The <strong>MDP framework</strong> is a considerable <strong>abstraction</strong> of the problem of goal-directed learning from interaction. It can be used to formalize a wide variety of <strong>sequential decision-making</strong> problem. This also means that any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment: actions, states, rewards.</p>
<h3 id="an-example-of-mdp">An Example of MDP</h3>
<p>Here is an example of finite MDP, <strong>Recycling Robot</strong>. We can write down the transition probabilities and the expected rewards, with dynamics as indicated below.<br />
<br />
We can show it in another useful way called <strong>trasition graph</strong><br />
<br />
There are two kinds of nodes: state nodes and action nodes. The key point here is that the agent fisrtly chooses an action and then transits to a state with corresponding probability.</p>
<hr />
<h2 id="goal-of-reinforcement-learning">Goal of Reinforcement Learning</h2>
<h3 id="the-goal-of-rl">The Goal of RL</h3>
<p>In reinforcement learning, reward captures the notion of short-term gain. The objective however, is to learn a policy that achieves the most reward in the long run. With this premise, we denote the <strong>return</strong> at time step <span class="math inline">\(t\)</span> as <span class="math inline">\(G_t\)</span></p>
<p><span class="math display">\[G_t\doteq R_{t+1}+R_{t+2}+R_{t+3}+...\tag{2.2}\]</span></p>
<p>We intuitively want to maxmize <span class="math inline">\(G_t\)</span> at each time step. However, <span class="math inline">\(G_t\)</span> is a random variable because <strong>the dynamics of the MDP can be stochastic</strong>. That's why we define the goal of an agent is to <strong>maxmize the expected return</strong></p>
<p><span class="math display">\[\mathbb{E}[G_t] = \mathbb{E}[R_{t+1}+R_{t+2}+R_{t+3}+...]\tag{2.3}\]</span></p>
<h3 id="the-reward-hypothesis">The Reward Hypothesis</h3>
<p>The <strong>Reinforcement Learning Hypothesis</strong> can be expressed as this:</p>
<blockquote>
<p>That all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).</p>
</blockquote>
<p>There are two research programs:</p>
<ul>
<li>Identify where reward signals come from. (what reward the agent should optimize)</li>
<li>Develop algorithms that search the space of action to maximize reward signals.</li>
</ul>
<p>Informally, the agent’s goal is to <strong>maximize the total amount of reward it receives</strong>. This means maximizing not immediate reward, but cumulative reward in the long run. We can clearly state this informal idea as the <strong>reward hypothesis</strong>:</p>
<blockquote>
<p>That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).</p>
</blockquote>
<p>But how can we define reward? This is a trick problem. To use goals as rewards, we have some representations:</p>
<ul>
<li>goal-reward representation: 1 for achieving goal, 0 otherwise</li>
<li>action-penalty representation: -1 for not achieving goal, 0 once goal reached</li>
</ul>
<p>Both of representations have its own limitation. They lack of some middel information to tell the agent how to achieve the goal as our expectation.</p>
<hr />
<h2 id="episodic-tasks-and-continuing-tasks">Episodic Tasks And Continuing Tasks</h2>
<h3 id="episodic-tasks">Episodic Tasks</h3>
<p>In an <strong>episodic task</strong>, the agent-environment interaction breaks up into <strong>episodes</strong>. Episodes are indepedent and each episode ends in a <strong>terminal state</strong> at time step <span class="math inline">\(T\)</span>. In this situation, time step is finite and we change <a href="#the-goal-of-rl">(2.2)</a> as</p>
<p><span class="math display">\[G_t \doteq R_{t+1}+R_{t+2}+R_{t+1}+...+R_T\tag{2.4}\]</span></p>
<h3 id="continuing-tasks">Continuing Tasks</h3>
<p>The interaction between agent-environment goes on <strong>continually</strong> and there is no <strong>terminal state</strong>. In this situation, time step is infinite and <span class="math inline">\(G_t\)</span> is the same in <span class="math inline">\((2.2)\)</span></p>
<p><span class="math display">\[G_t \doteq R_{t+1}+R_{t+2}+R_{t+3}+...\]</span></p>
<p>But here comes to one problem: <span class="math inline">\(G_t\)</span> could be <span class="math inline">\(\infty\)</span> and impossible to calculate. Therefore, we should make <span class="math inline">\(G_t\)</span> finite, which by using <strong>discounting</strong> method.</p>
<h3 id="discounting-return">Discounting Return</h3>
<p>Discount the rewards in the future by <span class="math inline">\(\gamma\)</span> called <strong>discount rate</strong>, where <span class="math inline">\(\gamma \in [0,1]\)</span></p>
<p><span class="math display">\[
\begin{eqnarray*}
G_t
&amp;\doteq&amp; R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+...+\gamma^{k-1}R_{t+k}+...\\
&amp;=&amp;\sum_{k=0}^\infty \gamma^kR_{t+k+1}\tag{2.5}
\end{eqnarray*}
\]</span></p>
<p>We can also re-write <span class="math inline">\(G_t\)</span> in the recursive version</p>
<p><span class="math display">\[G_t = R_{t+1}+\gamma G_{t+1}\tag{2.6}\]</span></p>
<p>To prove <span class="math inline">\(G_t\)</span> is finite, let's assume <span class="math inline">\(R_{max}\)</span> is the maximum reward the agent can receive</p>
<p><span class="math display">\[
\begin{eqnarray*}
G_t=\sum_{k=0}^\infty \gamma^kR_{t+k+1}&amp;\le&amp; \sum_{k=0}^\infty \gamma^kR_{max}\\
&amp;=&amp;R_{max}\sum_{k=0}^\infty \gamma^k\\
&amp;=&amp;R_{max}\times \frac{1}{1-\gamma}
\end{eqnarray*}
\]</span></p>
<p>It proves that <span class="math inline">\(G_t\)</span> now is finite. To Specify, let's talk about <span class="math inline">\(\gamma=0\)</span> and <span class="math inline">\(\gamma=1\)</span>.</p>
<ul>
<li><span class="math inline">\(\gamma=0\Rightarrow G_t=R_t\)</span> : agent only cares about immediate reward. (<strong>short-sighted agent</strong>)</li>
<li><span class="math inline">\(\gamma\rightarrow1\)</span> : agent takes future rewards into account more strongly. (<strong>far-sighted agent</strong>)</li>
</ul>
<h3 id="unified-episodic-and-continuing-tasks">Unified Episodic and Continuing Tasks</h3>
<p>we can consider episode termination to be the entering of a special <strong>absorbing state</strong> that transitions only to itself and that generates only rewards of zero. For example, consider the state transition diagram:<br />
<br />
Thus, we can informally regard epsoidic tasks as continuing task.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2020/02/27/RL1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/27/RL1/" class="post-title-link" itemprop="url">RL1：K-armed Bandits Problem</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-27 12:35:49" itemprop="dateCreated datePublished" datetime="2020-02-27T12:35:49-05:00">2020-02-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-17 11:19:28" itemprop="dateModified" datetime="2020-03-17T11:19:28-04:00">2020-03-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<blockquote>
<p>Refer from <em>Reinforcement Learning An Introduction (2nd)</em> and <em>Reinforcement Learning specialization on Coursera by University of Alberta &amp; Alberta Machine Intelligence Institute</em></p>
</blockquote>
<h2 id="the-k-armed-bandit-problem">The K-Armed Bandit Problem</h2>
<p>In the k-armed bandit problem, we have an <strong>agent</strong> who choooses between "k" <strong>action</strong> and recieves a <strong>reward</strong> based on the action it chooses. The <strong>goal</strong> of the agent is to maximize the expected total reward over some time period, or <strong>time steps</strong>.</p>
<p>In a word, <strong>decision making under uncertainty</strong> can be formalized by the <strong>k-armed bandit problem</strong></p>
<h3 id="action-values">Action Values</h3>
<p>the value of an action is the <strong>expected reward</strong> when that action is taken <span class="math display">\[
\begin{eqnarray*}
q_*(a) &amp;\doteq&amp; \mathbb{E}[R_t\ | \ A_t = a],\ \forall a \in \mathbf{A}\\
&amp;=&amp;\sum_rp(r|a)r\tag{1.1}
\end{eqnarray*}
\]</span> To clearly explain this, the reward of an action may not be fixed so the reward may comfirm to a uniform distribution or normal distribution. Here is an example to illustrate this process </p>
<h3 id="action-selection">Action Selection</h3>
<p>Remember that the agent's <strong>goal</strong> is to <strong>maximize the expected total amount of reward it recieves</strong>. We can call this procedure the <em>argmax</em> (the argument "a" which maximizes function <span class="math inline">\(q_*\)</span>) <span class="math display">\[\mathop{argmax}_a \ q_*(a)\]</span> It is clear that if you know the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the <strong>action with highest value</strong>.</p>
<p>Thus, the problem here is that the agent do not know the action values with certainty. It only have to estimate the action values. But how to estimate action-values?</p>
<hr />
<h2 id="what-to-learn-estimating-action-values">What to Learn? Estimating Action Values</h2>
<h3 id="sample-average">Sample-Average</h3>
<p>One way to estimate <span class="math inline">\(q_*\)</span> is to compute a <strong>sample-average</strong>. Here we denote the estimated value of action <span class="math inline">\(a\)</span> at time step <span class="math inline">\(t\)</span> as <span class="math inline">\(Q_t(a)\)</span>. We would like <span class="math inline">\(Q_t(a)\)</span> to be close to <span class="math inline">\(q_*(a)\)</span>.</p>
<p><span class="math display">\[Q_t(a) \doteq \frac{\sum_{i=1}^{t-1}R_i}{t-1}\tag{1.2}\]</span></p>
<p>Notice that we use <span class="math inline">\(t-1\)</span> because we are calculating <span class="math inline">\(Q_t(a)\)</span> with history prior to <span class="math inline">\(t\)</span>. And to be clear, <span class="math inline">\(t\)</span> has <strong>separate corresponding value for each action</strong>.<br />
Here is an example of medical trail. <br />
And we can simplify <span class="math inline">\(Q_t\)</span> by extending it in <strong>incremental update rule</strong></p>
<p><span class="math display">\[
\begin{eqnarray*}
Q_{t+1}
&amp;=&amp;\frac{1}{t}\sum_{i=1}^tR_i\\
&amp;=&amp;\frac{1}{t}(R_t+(t-1)\frac{1}{t-1}\sum_{i=1}^{t-1}R_i)\\
&amp;=&amp;\frac{1}{t}(R_t+(t-1)Q_t)\\
&amp;=&amp;Q_t+\frac{1}{t}(R_t-Q_t)\tag{1.3}
\end{eqnarray*}
\]</span></p>
<p>To generalize it, we replace <span class="math inline">\(\frac{1}{t}\)</span> with <span class="math inline">\(\alpha_t\)</span> to represent stepsize and it's easy to know <span class="math inline">\(\alpha_t \in [0,1]\)</span></p>
<p><span class="math display">\[Q_{t+1}=Q_t+\alpha_t(R_t-Q_t)\tag{1.4}\]</span></p>
<p>And it can also express as below</p>
<p><span class="math display">\[NewEstimate \leftarrow OldEstimate\ +\  StepSize(Target\ -\ OldEstimate)\]</span></p>
<p>The expression <span class="math inline">\((Target-OldEstimate)\)</span> is an <strong>error</strong> in the estimate. It is reduced by taking a step toward the “Target.” <strong>The target is presumed to indicate a desirable direction in which to move, though it may be noisy.</strong> In the case above, for example, the target is the nth reward.</p>
<h3 id="nonstationary-problem">Nonstationary Problem</h3>
<p>What if the reward distribution of each action is <strong>changing over time</strong>? In this situation, what we learn form history may not correctly represent current state due to changing of reward distribution. In such cases it makes sense to give more weight to recent rewards than to long-past rewards because recent information reflect changing more accurate. To do so, we can use a <strong>fixed stepsize</strong>,like 0.5, instead of <span class="math inline">\(\frac{1}{t}\)</span>. This can lead to decaying exponently past reward.</p>
<p><span class="math display">\[Q_{t+1}=Q_t+\alpha_t(R_t-Q_t)=\alpha_tR_t+(1-\alpha_t)Q_t\tag{1.5}\]</span></p>
<p>As <span class="math inline">\(\alpha_t\)</span> increases, the more <span class="math inline">\(R_t\)</span> contributes to <span class="math inline">\(Q_{t+1}\)</span> and the less for <span class="math inline">\(Q_t\)</span>.</p>
<h3 id="greedy-action">Greedy Action</h3>
<p>At each time step, the agent could alway choose the action with the highest action value estimated by history. And this is called <strong>greedy action</strong>.</p>
<p><span class="math display">\[a_g = \mathop{argmax}_a\ \ Q(a)\]</span></p>
<p>Seemingly we can through greedy action to archieve our goal, maxmizing expected total reward. But there is still a problem exists: how does the agent know its estimation about each action is correctly approximating to the real distribution? Greedy action is greatly impact by initial action value. Why does the agent know other actions are worse than greedy action even trying them only a few times?</p>
<p>I think the agent should spend some time trying other actions instead of greedy action. And this come to the <strong>exploration and exploitation tradeoff</strong></p>
<hr />
<h2 id="exploration-vs.-exploitation-tradeoff">Exploration vs. Exploitation Tradeoff</h2>
<h3 id="exploration-and-exploitation-dilemma">Exploration and Exploitation Dilemma</h3>
<ul>
<li>Exploration: <strong>improve</strong> knowledge for <strong>long-term</strong> benefit</li>
<li>Exploitation: <strong>exploit</strong> knowledge for <strong>short-term</strong> benefit</li>
</ul>
<p>The dilemma means: How do we choose when to exploit and when to explore? When we <strong>explore</strong>, we get more accurate estimates of our values. When we <strong>exploit</strong>, we might get more reward. We cannot however choose to do both simultaneously.</p>
<h3 id="epsilon-greedy-action-selection">Epsilon-Greedy Action Selection</h3>
<p>One very simple method for choosing between exploration and exploitation is to <strong>choose randomly</strong>. We could choose to exploit most of the time with a small chance of exploring. For instance, we could roll a dice.</p>
<p>This is called <strong><span class="math inline">\(\epsilon\)</span>-greedy action selection</strong>. We formulize it as below, where <span class="math inline">\(\epsilon \in [0,1]\)</span></p>
<p><span class="math display">\[
A_t\leftarrow
\begin{cases}
\mathop{argmax}_a Q_t(a)\ \ \ \ \ \ \ \ 1-\epsilon\\ \\
a\sim Uniform(\mathbf{A})\ \ \ \ \ \ \epsilon
\end{cases}
\]</span></p>

<p>Let's evaluate on 10-armed testbed<br />
<br />
We run each agent with different <span class="math inline">\(\epsilon\)</span> for 2000 independent runs, each run for 1000 time steps. Then we average 2000 runs to get the learning algorithm’s average behavior.<br />
</p>
<p>The advantage of <span class="math inline">\(\epsilon\)</span>-greedy over greedy methods depends on the task. If the reward variance is large, like 10, then more explorations to detect best action is important. More detections give the agent more confidence to pick up optimal action.</p>
<p>On the other hand, if the reward variance is zero, then greedy method would know the true value of each action after trying it once. Seemingly, in this case, greedy method would soon find the greedy action and it might perform best. But, this can not be guaranteed under nonstationary situations. Nonstation means the true values of the actions changed over time. In this case, exploration is needed to ensure that one of the nongreedy actions has not changed to become better than the greedy one.</p>
<h3 id="optimistic-initial-values">Optimistic Initial Values</h3>
<p><strong>Optimistic initial values</strong> encourage <strong>early exploration</strong>. In real programing, don't intialize all acion values defaut to 0. Here is an example to show algorithm's performance by using this method.<br />
<br />
Limitation of optimistic inital values:</p>
<ul>
<li>Optimistic intial values <strong>only drive early exploration</strong></li>
<li>They are not will suited for <strong>nonstationary problems</strong></li>
<li>We may not know what the <strong>optimistic values</strong> should be</li>
</ul>
<h3 id="upper-confidence-bound-ucb-action-selection">Upper-Confidence Bound (UCB) Action Selection</h3>
<p><span class="math inline">\(\epsilon\)</span>-greedy action selection forces the non-greedy actions to be tried, but indiscriminately and <strong>uniformly</strong>, with no preference for those that are nearly greedy or particularly uncertain. It would be better to select among the non-greedy actions according to their <strong>potential for actually being optimal</strong>, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates.</p>
<p>In <strong>upper-confidence bound</strong>(UCB) action selection, we follow the principle of optimism in the face of uncertainty. This simply means that if we are uncertain about something, we should optimistically assume that it is good.</p>
<p>For instance, say we have these three actions with associated uncertainties. The larger confidence interval means bigger uncertainies. So the agent optimistically picks the action that has the highest upper bound, action 1. <br />
After that, this time, we should select action 2.<br />
<br />
The key idea in UCB is that it uses <strong>uncertainty</strong> or <strong>variance</strong> (sqrt root) in the value estimation for balancing exploration and exploitation.</p>
<p><span class="math display">\[A_t \doteq \mathop{argmax}_a [Q_t(a)+c\sqrt{\frac{\ln{t}}{N_t(a)}}\ ]\]</span></p>
<p>We can clearly see here how UCB combines exploration and exploitation. The first term in the sum represents the exploitation part, and the second term represents the exploration part. The constant parameter <span class="math inline">\(c\)</span> is used to scale the extent of exploration. <span class="math inline">\(N_t(a)\)</span> denotes the number of times that action <span class="math inline">\(a\)</span> has been selected prior to time <span class="math inline">\(t\)</span>.<br />
<br />
The limitation of UCB is <strong>not practical for very huge state spaces</strong>.</p>
<h2 id="summary">Summary</h2>
<p>The gap between k-armed bandit problem and the full reinforcement learning problem:</p>
<ol type="1">
<li>there is <strong>no association</strong> between actions and situations(or <strong>states</strong>). Each selection of actions is <strong>non-contextual</strong> or independent.</li>
<li>thers is <strong>no policy</strong>, which designate actions at each situation(or <strong>state</strong>).</li>
</ol>
<p>we summarize a complete learning curve by its average value over the 1000 steps; this value is proportional to the area under the learning curve. This kind of graph is called a <strong>parameter study</strong>. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2020/02/25/Game2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/25/Game2/" class="post-title-link" itemprop="url">博弈论2：纯策略纳什均衡</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-25 11:23:24" itemprop="dateCreated datePublished" datetime="2020-02-25T11:23:24-05:00">2020-02-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-21 21:43:02" itemprop="dateModified" datetime="2020-03-21T21:43:02-04:00">2020-03-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Game-Theory/" itemprop="url" rel="index"><span itemprop="name">Game Theory</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<blockquote>
<p>All materials come from <em>Coursera</em>《Game Theory》by Stanford University &amp; The University of British Columbia</p>
</blockquote>
<h2 id="intro-to-nash-equilibrium">Intro to nash equilibrium</h2>
<p>Attributes of nash equilibrium</p>
<ul>
<li><strong>nobody has an incentive to </strong>deviate** from their actions if a nash equilibrium is achieved**</li>
<li>each player's action <strong>maximizes his/her payoff</strong> given the actions of others</li>
<li>a self-consistent or <strong>stable</strong> profile</li>
</ul>
<h2 id="pure-strategy-nash-equilibrium">Pure Strategy Nash Equilibrium</h2>
<h3 id="pure-strategy-and-mixed-strategy">Pure Strategy And Mixed Strategy</h3>
<p>the informal definitions of <strong>pure strategy</strong> and <strong>mixed strategy</strong> are</p>
<ul>
<li>pure strategy: <strong>only one</strong> action is played with positive probability</li>
<li>mixed strategy: <strong>more than one</strong> action is played with positive probability</li>
</ul>
<p>With a pure strategy, a player choose one action with 100% ensurance. While with a mixed strategy, a player can have a probability of 50% choosing action A and a probability of 50% choosing action B. In another word, player <span class="math inline">\(i\)</span> chooses action <span class="math inline">\(a_i\)</span> randomly from <span class="math inline">\(A\)</span> with corresponding probability.</p>
<h3 id="best-response">Best Response</h3>
<p>The idea of <strong>Best Response</strong>: if you know what everyone else was going to do, it would be easy to pick your own action.</p>
<p>Based on the previous lecture, <span class="math inline">\(a\)</span> represents <strong>action profile</strong>, which organized by <span class="math inline">\(n\)</span> players' action</p>
<p><span class="math display">\[a = (a_1, a_2,..., a_n)\]</span></p>
<p>Now we denote <span class="math inline">\(a_{-i}\)</span>, which means a sub action profile of other players act</p>
<p><span class="math display">\[a_{-i} = (a_1, a_2,..., a_{i-1}, a_{i+1},..., a_n)\]</span></p>
<p>Then we can re-define <span class="math inline">\(a\)</span> as</p>
<p><span class="math display">\[a = (a_i, a_{-i})\]</span></p>
<p>Therefore, the definition of <strong>Best Response</strong> is showen below</p>
<p><span class="math display">\[a_i^* \in BR(a_{-i}) \ \ iff \ \ \forall a_i \in A_i, u_i(a_i^*, a_{-i}) \ge u_i(a_i, a_{-i})\]</span></p>
<p>That is to say, given <span class="math inline">\(a_{-i}\)</span>, <span class="math inline">\(a_i^*\)</span> is the best response for player <span class="math inline">\(i\)</span> to play.</p>
<p>But in practice, no agent knows what the others will do, so we can't say about what actions will actually occur. The idea here is to look for <strong>stable action profiles</strong>, leading to equilibrium point. <span class="math inline">\(a = (a_1, a_2,..., a_n)\)</span> is a (pure strategy) <strong>nash equilibrium</strong> iff <span class="math inline">\(\forall i,\ a_i \in BR(a_{-i})\)</span></p>
<h3 id="dominant-strategy">Dominant Strategy</h3>
<p>generalize from actions to strategies</p>
<h2 id="mixed-strategy-nash-equilibrium">Mixed Strategy Nash Equilibrium</h2>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2020/02/05/Others4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/05/Others4/" class="post-title-link" itemprop="url">Linux下常用指令</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-05 14:46:11" itemprop="dateCreated datePublished" datetime="2020-02-05T14:46:11-05:00">2020-02-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-29 04:03:22" itemprop="dateModified" datetime="2020-02-29T04:03:22-05:00">2020-02-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Others/" itemprop="url" rel="index"><span itemprop="name">Others</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="top">top</h2>
<p>查看系统进程</p>
<h2 id="screen">screen</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">screen -S [TaskName] 创建一个名为TaskName的screen</span><br><span class="line">screen -r(-x) [TaskName] 恢复（进入）名为TaskName的screen</span><br></pre></td></tr></table></figure>
<p>快捷指令：</p>
<ul>
<li>上下分屏：ctrl + a 再按shift + s</li>
<li>切换屏幕：ctrl + a 再按tab键</li>
<li>新建一个终端：ctrl + a 再按c</li>
</ul>
<h2 id="dudf">du、df</h2>
<p>du = disk used 硬盘使用情况 df = disk free 硬盘剩余空间</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">du -h [FileName] 查看指定文件大小</span><br><span class="line"></span><br><span class="line">df -hl 查看磁盘剩余空间</span><br><span class="line"></span><br><span class="line">df -h 查看每个根路径的分区大小</span><br><span class="line"></span><br><span class="line">du -sh [目录名] 返回该目录的大小</span><br><span class="line"></span><br><span class="line">du -sm [文件夹] 返回该文件夹总M数</span><br></pre></td></tr></table></figure>
<h2 id="find">find</h2>
<p>find命令是根据文件的属性进行查找，如文件名，文件大小，所有者，所属组，是否为空，访问时间，修改时间等</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find [location] -name [FileName] 在location下查找名字为FileName的文件</span><br></pre></td></tr></table></figure>
<h2 id="grep">grep</h2>
<p>grep是根据文件的内容进行查找，会对文件的每一行按照给定的模式(patter)进行匹配查找</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep [字符串] [File] 在File中按行匹配字符串</span><br></pre></td></tr></table></figure>
<h2 id="whichwhereis">which、whereis</h2>
<p>which 查看可执行文件的位置 ，只有设置了环境变量的程序才可以用 whereis 寻找特定文件，只能用于查找二进制文件、源代码文件和man手册页</p>
<h2 id="moreless">more/less</h2>
<p>查看文件内容，当文件内容很大时比cat命令要好用</p>
<h2 id="headtail">head、tail</h2>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">head(tail) -n 1000 [FileName] 显示FileName文件的前1000行(后1000行)</span><br><span class="line">tail -n +1000 [FileName] 显示FileName文件从1000行往后的内容</span><br><span class="line">tail -f [FileName] 追踪（follow）文件FileName</span><br></pre></td></tr></table></figure>
<h2 id="freeswapoffmkswapswapon">free、swapoff、mkswap、swapon</h2>
<p>查看内存状态命令，可以显示memory、swap、buffer/cache等的大小及使用状况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">free -h 查看内存状态</span><br><span class="line"></span><br><span class="line">swapoff -a 取消所有挂载的swap空间</span><br><span class="line"></span><br><span class="line">dd if=/dev/zero of=/var/swapfile bs=1M count=1024 在/var目录下创建大小为1G的文件用做swap空间</span><br><span class="line"></span><br><span class="line">mkswap /var/swapfile 格式化swap文件</span><br><span class="line"></span><br><span class="line">swapon /var/swapfile 挂载swap空间</span><br></pre></td></tr></table></figure>
<h2 id="netstat">netstat</h2>
<p>查看网络状态（地址和端口号）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -tunlp 其中t（tcp）、u（udp），nlp分别是三个flag参数</span><br></pre></td></tr></table></figure>
<h2 id="tmux">tmux</h2>
<p><a target="_blank" rel="noopener" href="http://www.ruanyifeng.com/blog/2019/10/tmux.html">详细内容参考</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tmux new -s [NewSessionName] 创建标签为NewSessionName的新会话，默认数字index从0开始</span><br><span class="line">tmux attach -t [index/name] 恢复到index/name的会话连接</span><br><span class="line">tmux ls 查看所有会话</span><br><span class="line">tmux switch -t [index/name] 切换到index/name会话</span><br></pre></td></tr></table></figure>
<p>快捷指令 ：</p>
<ul>
<li>ctrl + b %： 左右分屏</li>
<li>ctrl + b “： 上下分屏</li>
<li>ctrl + b 空格： 左右分屏</li>
<li>ctrl + b 上下左右： 切换分屏</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Godway"
      src="/images/avatar.ico">
  <p class="site-author-name" itemprop="name">Godway</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">35</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="/475865836@qq.com" title="E-Mail → 475865836@qq.com"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://randool.cn/" title="https:&#x2F;&#x2F;randool.cn" rel="noopener" target="_blank">Randool</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://me.csdn.net/Smile_coderrr" title="https:&#x2F;&#x2F;me.csdn.net&#x2F;Smile_coderrr" rel="noopener" target="_blank">Smile_coderrr</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Godway</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
