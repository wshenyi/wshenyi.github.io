<!DOCTYPE html>
<html lang="">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon_32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon_16x16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wshenyi.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":300,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Godway&#39;s Notebook">
<meta property="og:url" content="https://wshenyi.github.io/page/4/index.html">
<meta property="og:site_name" content="Godway&#39;s Notebook">
<meta property="og:locale">
<meta property="article:author" content="Godway">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://wshenyi.github.io/page/4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'default'
  };
</script>

  <title>Godway's Notebook</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Godway's Notebook</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">wsy</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/WSHENYI/WSHENYI.github.io" class="github-corner" title="Star me on GitHub" aria-label="Star me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2020/02/29/RL3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/29/RL3/" class="post-title-link" itemprop="url">RL3：Value Functions & Bellman Equations</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-29 11:14:01" itemprop="dateCreated datePublished" datetime="2020-02-29T11:14:01-05:00">2020-02-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-17 11:19:49" itemprop="dateModified" datetime="2020-03-17T11:19:49-04:00">2020-03-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<blockquote>
<p>Refer from <em>Reinforcement Learning An Introduction (2nd)</em> and <em>Reinforcement Learning specialization on Coursera by University of Alberta &amp; Alberta Machine Intelligence Institute</em></p>
</blockquote>
<h2 id="Policies-and-Value-Functions"><a href="#Policies-and-Value-Functions" class="headerlink" title="Policies and Value Functions"></a>Policies and Value Functions</h2><h3 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h3><p>Formally, a <strong>policy</strong> is a mapping from <strong>states</strong> to probabilities of selecting each possible <strong>action</strong>.</p>
<ul>
<li><strong>deterministic policy</strong>, denote $\pi(s)=a$: one state correspond to one action</li>
<li><strong>stochastic policy</strong>, $\pi(a|s)$: a probability distribution over all visable actions</li>
</ul>
<img src="/2020/02/29/RL3/1.png" class="">
<img src="/2020/02/29/RL3/2.png" class="">

<p>It’s important that <strong>policies depend only on the current state</strong>, not on other things like time or previous states.</p>
<h3 id="State-value-Function"><a href="#State-value-Function" class="headerlink" title="State-value Function"></a>State-value Function</h3><p>Almost all reinforcement learning algorithms involve <strong>estimating value functions</strong> – functions of states (or of state–action pairs) that estimate <strong>how good</strong> it is for the agent to be in a given state (or <strong>how good</strong> it is to perform a given action in a given state), which means value function predicts reward into future.</p>
<p>The value function of a state $s$ under a policy $\pi$, denoted $v_\pi(s)$, is <strong>the expected return when starting in $s$ and following $\pi$ thereafter</strong>. For MDPs, we can define $v_\pi$ formally by</p>
<p>$$v_\pi(s)\doteq \mathbb{E}<em>\pi[G_t|S_t=s]=\mathbb{E}<em>\pi[\sum</em>{k=0}^\infty \gamma^kR</em>{t+k+1}|S_t=s],\ for\ all\ s\in \mathbf{S}$$</p>
<p>We call the function $v_\pi$ the <strong>state-value function for policy $\pi$</strong></p>
<h3 id="Action-value-Function"><a href="#Action-value-Function" class="headerlink" title="Action-value Function"></a>Action-value Function</h3><p>Similarly, we define the value of taking action $a$ in state $s$ under a policy $\pi$, denoted $q_\pi(s,a)$, as <strong>the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$</strong>:</p>
<p>$$q_\pi(s,a)\doteq \mathbb{E}<em>\pi[G_t|S_t=s,A_t=a]=\mathbb{E}<em>\pi[\sum</em>{k=0}^\infty \gamma^kR</em>{t+k+1}|S_t=s,A_t=a]$$</p>
<p>We call the function $q_\pi$ the <strong>action-value function for policy \pi</strong>,</p>
<hr>
<h2 id="Bellman-Equations"><a href="#Bellman-Equations" class="headerlink" title="Bellman Equations"></a>Bellman Equations</h2><h3 id="Transform-Between-v-pi-And-q-pi"><a href="#Transform-Between-v-pi-And-q-pi" class="headerlink" title="Transform Between $v_\pi$ And $q_\pi$"></a>Transform Between $v_\pi$ And $q_\pi$</h3><p>Through the given diagram, we can derivate an equation for $v_\pi$ in terms of $q_\pi$ and $\pi$.</p>
<img src="/2020/02/29/RL3/6.png" class="">

<p>$$<br>\begin{eqnarray*}<br>v_\pi(s)<br>&amp;=&amp;\mathbb{E}<em>\pi[G_t|S_t=s]\<br>&amp;=&amp;\sum_a\pi(a|s)\mathbb{E}[G_t|S_t=s,A_t=a]\<br>&amp;=&amp;\sum_a\pi(a|s)q(s,a)\tag{3.1}<br>\end{eqnarray*}<br>$$<br>Recall that $G_t=R</em>{t+1}+\gamma G_{t+1}$. With given diagram below, we can derivate an equation for $q_\pi$ in terms of $v_\pi$ and $p(s’,r|s,a)$. (use markov property to simplify original equation)</p>
<img src="/2020/02/29/RL3/7.png" class="">

<p>$$<br>\begin{eqnarray*}<br>q_\pi(s,a)<br>&amp;=&amp;\mathbb{E}<em>\pi[G_t|S_t=s,A_t=a]\<br>&amp;=&amp;\mathbb{E}<em>\pi[R</em>{t+1}+\gamma G</em>{t+1}]\<br>&amp;=&amp;\sum_{s’,r}p(s’,r|s,a)(r+\gamma \mathbb{E}<em>\pi[G</em>{t+1}|S_{t+1}=s’])\<br>&amp;=&amp;\sum_{s’,r}p(s’,r|s,a)(r+\gamma v_\pi(s’))\tag{3.2}<br>\end{eqnarray*}<br>$$</p>
<h3 id="Bellman-Equations-for-v-pi"><a href="#Bellman-Equations-for-v-pi" class="headerlink" title="Bellman Equations for $v_\pi$"></a>Bellman Equations for $v_\pi$</h3><p><strong>Bellamn equation allows us to relate the value of the current state to the value of future states without waiting to observe all the future rewards.</strong><br>With $(3.1),(3.2)$, we can easiy get bellamn equation for $v_\pi$</p>
<p>$$<br>\begin{eqnarray*}<br>v_\pi(s)<br>&amp;=&amp;\sum_a\pi(a|s)q(s,a)\<br>&amp;=&amp;\sum_a\pi(a|s)\sum_{s’,r}p(s’,r|s,a)(r+\gamma v_\pi(s’))\tag{3.3}<br>\end{eqnarray*}<br>$$</p>
<p>$s’$ here stands for next state. <strong>Bellamn equation expresses a relationship between the value of a state and the values of its successor states (next state).</strong>  You can get a more intiutive understanding about how this equation forms though its backup diagram below: <strong>we first choose actions through policy and then transites to a successor state with corresponding reward under environment dynamics</strong>. The open circles represent state nodes and solid circles represent action nodes.</p>
<img src="/2020/02/29/RL3/4.png" class="">

<h3 id="Bellman-Equations-For-q-pi"><a href="#Bellman-Equations-For-q-pi" class="headerlink" title="Bellman Equations For $q_\pi$"></a>Bellman Equations For $q_\pi$</h3><p>With $(3.1),(3.2)$, we can easiy get bellamn equation for $q_\pi$</p>
<p>$$<br>\begin{eqnarray*}<br>q_\pi(s,a)<br>&amp;=&amp;\sum_{s’,r}p(s’,r|s,a)(r+\gamma v_\pi(s’))\<br>&amp;=&amp;\sum_{s’,r}p(s’,r|s,a)(r+\gamma\sum_{a’} q_\pi(s’,a’))\tag{3.4}<br>\end{eqnarray*}<br>$$</p>
<p>$a’$ here stands for actions under possible state $s’$. Similarly, <strong>this equation express a relationship betweeen the value of an action and the values of actions under possible successor state $s’$</strong></p>
<img src="/2020/02/29/RL3/5.png" class="">

<h3 id="An-Example-of-Value-Function-Grideworld"><a href="#An-Example-of-Value-Function-Grideworld" class="headerlink" title="An Example of Value Function: Grideworld"></a>An Example of Value Function: Grideworld</h3><ul>
<li><strong>State</strong>: each grid (totally 25 states)</li>
<li><strong>Action</strong>: north, south, east, and west</li>
<li><strong>Reward</strong>: actions that would take the agent off the grid leave its location unchanged, but also result in a reward of -1. Other actions results in a reward 0. Every time the agent moves any directions at position A would be transited to A’ with reward +10. This is the same to B and B’ except for reward +5.</li>
<li><strong>Policy</strong>: random policy, each direction has equal probability, 25%.</li>
</ul>
<p>$\gamma=0.9$</p>
<img src="/2020/02/29/RL3/3.png" class="">

<p>Now, just for curious, we can verify the $v_\pi(s)$ bellman equation in this example. Let’s say we choose center state.</p>
<p>$$(0.7\times0.9+0)\times0.25+(2.3\times0.9+0)\times0.25+(0.4\times0.9+0)\times0.25\+(-0.4\times0.9+0)\times0.25=0.69\approx0.7$$</p>
<h3 id="Why-Bellman-Equations"><a href="#Why-Bellman-Equations" class="headerlink" title="Why Bellman Equations?"></a>Why Bellman Equations?</h3><p>We use bellman equations to calculate $v_\pi(s)$ for all $s\in \mathbf{S}$ by solving the system of linear equations.</p>
<p>Still the previous example, gride world. It has 25 states and we get 25 unknown parameters. For each state we can get a bellman equation, therefore we get 25 equations. Through 25 equations, we sovle them to obtain 25 state values.</p>
<p>But bellman equation is not pragmatic for many reasons. One of them is that sometimes the number of states is too large and it’s impossible to calculate results in a short time. It can <strong>only solve small MDPs</strong>.</p>
<hr>
<h2 id="Optimality-Optimal-Policies-amp-Value-Functions"><a href="#Optimality-Optimal-Policies-amp-Value-Functions" class="headerlink" title="Optimality (Optimal Policies &amp; Value Functions)"></a>Optimality (Optimal Policies &amp; Value Functions)</h2><h3 id="Optimal-Policy"><a href="#Optimal-Policy" class="headerlink" title="Optimal Policy"></a>Optimal Policy</h3><p>A policy $\pi$ is defined to be better than or equal to a policy $\pi’$ if its expected return is greater than or equal to that of $\pi’$ for all states. In other words,</p>
<p>$$\pi&gt;\pi’ \ \ iff\ v_\pi(s)\ge v_{\pi’}(s)\ for\ all\ s \in \mathbf{S}$$</p>
<img src="/2020/02/29/RL3/8.png" class="">

<p><strong>There is always at least one deterministic policy that is better than or equal to all other policies. This is an optimal policy.</strong> Although <strong>there may be more than one</strong>, we denote all the optimal policies by $\pi_*$. They share the same state-value function, called the <strong>optimal state-value function</strong>, denoted $v_*$, and the same <strong>optimal action-value function</strong>, denoted $q_*$</p>
<p>$$v_*(s)\doteq \mathop{max}<em>\pi\ v_\pi(s)\<br>q</em>*(s,a)\doteq \mathop{max}_\pi\ q_\pi(s,a)\<br>for\ all\ s\in \mathbf{S},a\in \mathbf{A}$$</p>
<h3 id="Optimal-Value-Functions"><a href="#Optimal-Value-Functions" class="headerlink" title="Optimal Value Functions"></a>Optimal Value Functions</h3><p>Because the optimal policy $\pi_*$ is a <strong>deterministic policy</strong>, the optimal action $a_*$ with $\pi(a_*|s)$ equals to 1 and all other actions’ probability equal to 0. We can re-write $v_*$ with bellman equation as below, called <strong>bellman optimality equation for $v_*$</strong>.</p>
<p>$$<br>\begin{eqnarray*}<br>v_*(s)<br>&amp;=&amp;\mathop{max}<em>\pi\ v_\pi(s)\<br>&amp;=&amp;\mathop{max}<em>a \mathbb{E}[R</em>{t+1}+\gamma v</em><em>(S_{t+1})|S_t=s,A_t=a]\<br>&amp;=&amp;\mathop{max}<em>a\ \sum</em>{s’,r}p(s’,r|s,a)(r+\gamma v_</em>(s’))\tag{3.5}<br>\end{eqnarray*}<br>$$</p>
<p>Similarly, <strong>bellman optimality equation for $q_*$</strong> is</p>
<p>$$<br>\begin{eqnarray*}<br>q_*(s,a)<br>&amp;=&amp;\mathop{max}<em>\pi\ q_\pi(s,a)\<br>&amp;=&amp;\mathbb{E}[R</em>{t+1}+\gamma \mathop{max}<em>{a’}\ q</em><em>(S_{t+1},a’)|S_t=s,A_t=a]\<br>&amp;=&amp;\sum_{s’,r}p(s’,r|s,a)(r+\gamma \mathop{max}<em>{a’}\ q</em></em>(s’,a’))\tag{3.6}<br>\end{eqnarray*}<br>$$</p>
<img src="/2020/02/29/RL3/9.png" class="">

<p>The <strong>bellman optimality equations</strong> relate the value of a state or state-action pair, to it’s possible successors under <strong>any optimal policy</strong>.</p>
<p>Unfortunately, the $max$ funtion here is <strong>not linear</strong>, we <strong>cannot through linear system solver</strong> to directly solve bellman optimality equation to obtain all $v_*$ in MDPs. Although, in principle, one can use a variety of methods for solving systems of nonlinear equations, it is not recommended due to its extreme computational cost.</p>
<h3 id="Get-Optimal-Policy"><a href="#Get-Optimal-Policy" class="headerlink" title="Get Optimal Policy"></a>Get Optimal Policy</h3><p>Once one has $v_*$, it is relatively easy to determine an optimal policy. Because, for each state $s$, there will be one or more actions at which the maximum is obtained in the Bellman optimality equation. That is to say <strong>any policy that is greedy with respect to the optimal evaluation function $v_*$ is an optimal policy</strong>.</p>
<img src="/2020/02/29/RL3/10.png" class="">

<p>Having $q_*$ makes choosing optimal actions even easier. The action-value function effectively caches the results of all one-step-ahead searches.</p>
<img src="/2020/02/29/RL3/11.png" class="">

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2020/02/28/RL2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/28/RL2/" class="post-title-link" itemprop="url">RL2：Markov Decision Processes</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-28 17:21:31" itemprop="dateCreated datePublished" datetime="2020-02-28T17:21:31-05:00">2020-02-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-03 03:34:00" itemprop="dateModified" datetime="2020-04-03T03:34:00-04:00">2020-04-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<blockquote>
<p>Refer from <em>Reinforcement Learning An Introduction (2nd)</em> and <em>Reinforcement Learning specialization on Coursera by University of Alberta &amp; Alberta Machine Intelligence Institute</em></p>
</blockquote>
<h2 id="Introduction-to-Markov-Decision-Processes"><a href="#Introduction-to-Markov-Decision-Processes" class="headerlink" title="Introduction to Markov Decision Processes"></a>Introduction to Markov Decision Processes</h2><h3 id="Definition-of-MDP"><a href="#Definition-of-MDP" class="headerlink" title="Definition of MDP"></a>Definition of MDP</h3><p>MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal. Here are the key element in discribing a MDP</p>
<ul>
<li><strong>agent</strong>: the learner and the decision maker</li>
<li><strong>environment</strong>: the thing a agent interacts with, comprising everything outside the agent</li>
<li><strong>action</strong>: selected by the agent to interact with the environment</li>
<li><strong>reward</strong>: special numerical values that the agent seeks to maximize over time through its choice of actions.</li>
</ul>
<p>the whole interacting process is precisely simplified as follow  </p>
<img src="/2020/02/28/RL2/1.png" class="">  
<p>More specifically, the agent and the environment interact at each of a sequence of discrete time steps, $t=0,1,2,3,…$ At each time step $t$, the agent recives some representation of the environment’s <strong>state</strong>, $S_t \in \mathbf{S}$, and on that basis selects an <strong>action</strong>, $A_t \in \mathbf{A}(s)$.</p>
<p>One time step later, in part as a consequence of its action, the agent receives a numerical reward, $R_{t+1} \in \mathbf{R}\subset \mathbb{R}$, and finds itself in a new state, $S_{t+1}$. The MDP and agent together thereby give rise to a sequence or trajectory that begins like this:  </p>
<img src="/2020/02/28/RL2/2.png" class="">  
<p>In a finite MDP, the sets of states, actions, and rewards ($\mathbf{S}$, $\mathbf{A}$, and $\mathbf{R}$) all have a finite number of elements. In this case, the random variables $R_t$ and $S_t$ have well defined discrete probability distributions dependent only on the preceding state and action. That is, for particular values of these random variables, $s’ \in \mathbf{S}$ and $r \in \mathbf{R}$, there is a probability of those values occurring at time t, given particular values of the preceding state and action:<br>$$p(s’,r|s,a)\doteq P(S_t=s’, R_t=r\ | \ S_{t-1}=s, A_{t-1}=a)\tag{2.1}$$<br>for all $s’,s \in \mathbf{S}$, $r \in \mathbf{R}$, and $a \in \mathbf{A}(s)$. The function $p$ defines the <strong>dynamics of the MDP</strong>.<br>$$p:\ \mathbf{S}\times \mathbf{R}\times \mathbf{S}\times \mathbf{A}\rightarrow [0,1]$$<br>With $p(s’,r|s,a)$, note that <strong>future state and reward only depends on the current state and action</strong>. This is called the <strong>Markov property</strong>. It means that the present state is sufficient and remembering earlier states would not improve predictions about the future.</p>
<p>The <strong>MDP framework</strong> is a considerable <strong>abstraction</strong> of the problem of goal-directed learning from interaction. It can be used to formalize a wide variety of <strong>sequential decision-making</strong> problem. This also means that any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment: actions, states, rewards.</p>
<h3 id="An-Example-of-MDP"><a href="#An-Example-of-MDP" class="headerlink" title="An Example of MDP"></a>An Example of MDP</h3><p>Here is an example of finite MDP, <strong>Recycling Robot</strong>. We can write down the transition probabilities and the expected rewards, with dynamics as indicated below.  </p>
<img src="/2020/02/28/RL2/3.png" class="">  
<p>We can show it in another useful way called <strong>trasition graph</strong>  </p>
<img src="/2020/02/28/RL2/4.png" class="">  
<p>There are two kinds of nodes: state nodes and action nodes. The key point here is that the agent fisrtly chooses an action and then transits to a state with corresponding probability.</p>
<hr>
<h2 id="Goal-of-Reinforcement-Learning"><a href="#Goal-of-Reinforcement-Learning" class="headerlink" title="Goal of Reinforcement Learning"></a>Goal of Reinforcement Learning</h2><h3 id="The-Goal-of-RL"><a href="#The-Goal-of-RL" class="headerlink" title="The Goal of RL"></a>The Goal of RL</h3><p>In reinforcement learning, reward captures the notion of short-term gain. The objective however, is to learn a policy that achieves the most reward in the long run. With this premise, we denote the <strong>return</strong> at time step $t$ as $G_t$</p>
<p>$$G_t\doteq R_{t+1}+R_{t+2}+R_{t+3}+…\tag{2.2}$$</p>
<p>We intuitively want to maxmize $G_t$ at each time step. However, $G_t$ is a random variable because <strong>the dynamics of the MDP can be stochastic</strong>. That’s why we define the goal of an agent is to <strong>maxmize the expected return</strong></p>
<p>$$\mathbb{E}[G_t] = \mathbb{E}[R_{t+1}+R_{t+2}+R_{t+3}+…]\tag{2.3}$$</p>
<h3 id="The-Reward-Hypothesis"><a href="#The-Reward-Hypothesis" class="headerlink" title="The Reward Hypothesis"></a>The Reward Hypothesis</h3><p>The <strong>Reinforcement Learning Hypothesis</strong> can be expressed as this:</p>
<blockquote>
<p>That all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).</p>
</blockquote>
<p>There are two research programs:</p>
<ul>
<li>Identify where reward signals come from. (what reward the agent should optimize)</li>
<li>Develop algorithms that search the space of action to maximize reward signals.</li>
</ul>
<p>Informally, the agent’s goal is to <strong>maximize the total amount of reward it receives</strong>. This means maximizing not immediate reward, but cumulative reward in the long run. We can clearly state this informal idea as the <strong>reward hypothesis</strong>:</p>
<blockquote>
<p>That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).</p>
</blockquote>
<p>But how can we define reward? This is a trick problem. To use goals as rewards, we have some representations:</p>
<ul>
<li>goal-reward representation: 1 for achieving goal, 0 otherwise</li>
<li>action-penalty representation: -1 for not achieving goal, 0 once goal reached</li>
</ul>
<p>Both of representations have its own limitation. They lack of some middel information to tell the agent how to achieve the goal as our expectation.</p>
<hr>
<h2 id="Episodic-Tasks-And-Continuing-Tasks"><a href="#Episodic-Tasks-And-Continuing-Tasks" class="headerlink" title="Episodic Tasks And Continuing Tasks"></a>Episodic Tasks And Continuing Tasks</h2><h3 id="Episodic-Tasks"><a href="#Episodic-Tasks" class="headerlink" title="Episodic Tasks"></a>Episodic Tasks</h3><p>In an <strong>episodic task</strong>, the agent-environment interaction breaks up into <strong>episodes</strong>. Episodes are indepedent and each episode ends in a <strong>terminal state</strong> at time step $T$. In this situation, time step is finite and we change <a href="#the-goal-of-rl">(2.2)</a> as</p>
<p>$$G_t \doteq R_{t+1}+R_{t+2}+R_{t+1}+…+R_T\tag{2.4}$$</p>
<h3 id="Continuing-Tasks"><a href="#Continuing-Tasks" class="headerlink" title="Continuing Tasks"></a>Continuing Tasks</h3><p>The interaction between agent-environment goes on <strong>continually</strong> and there is no <strong>terminal state</strong>. In this situation, time step is infinite and $G_t$ is the same in $(2.2)$</p>
<p>$$G_t \doteq R_{t+1}+R_{t+2}+R_{t+3}+…$$</p>
<p>But here comes to one problem: $G_t$ could be $\infty$ and impossible to calculate. Therefore, we should make $G_t$ finite, which by using <strong>discounting</strong> method.</p>
<h3 id="Discounting-Return"><a href="#Discounting-Return" class="headerlink" title="Discounting Return"></a>Discounting Return</h3><p>Discount the rewards in the future by $\gamma$ called <strong>discount rate</strong>, where $\gamma \in [0,1]$</p>
<p>$$<br>\begin{eqnarray*}<br>G_t<br>&amp;\doteq&amp; R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+…+\gamma^{k-1}R_{t+k}+…\<br>&amp;=&amp;\sum_{k=0}^\infty \gamma^kR_{t+k+1}\tag{2.5}<br>\end{eqnarray*}<br>$$</p>
<p>We can also re-write $G_t$ in the recursive version</p>
<p>$$G_t = R_{t+1}+\gamma G_{t+1}\tag{2.6}$$</p>
<p>To prove $G_t$ is finite, let’s assume $R_{max}$ is the maximum reward the agent can receive</p>
<p>$$<br>\begin{eqnarray*}<br>G_t=\sum_{k=0}^\infty \gamma^kR_{t+k+1}&amp;\le&amp; \sum_{k=0}^\infty \gamma^kR_{max}\<br>&amp;=&amp;R_{max}\sum_{k=0}^\infty \gamma^k\<br>&amp;=&amp;R_{max}\times \frac{1}{1-\gamma}<br>\end{eqnarray*}<br>$$</p>
<p>It proves that $G_t$ now is finite. To Specify, let’s talk about $\gamma=0$ and $\gamma=1$.</p>
<ul>
<li>$\gamma=0\Rightarrow G_t=R_t$ : agent only cares about immediate reward. (<strong>short-sighted agent</strong>)</li>
<li>$\gamma\rightarrow1$ : agent takes future rewards into account more strongly. (<strong>far-sighted agent</strong>)</li>
</ul>
<h3 id="Unified-Episodic-and-Continuing-Tasks"><a href="#Unified-Episodic-and-Continuing-Tasks" class="headerlink" title="Unified Episodic and Continuing Tasks"></a>Unified Episodic and Continuing Tasks</h3><p>we can consider episode termination to be the entering of a special <strong>absorbing state</strong> that transitions only to itself and that generates only rewards of zero. For example, consider the state transition diagram:  </p>
<img src="/2020/02/28/RL2/5.png" class="">  
<p>Thus, we can informally regard epsoidic tasks as continuing task.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2020/02/27/RL1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/27/RL1/" class="post-title-link" itemprop="url">RL1：K-armed Bandits Problem</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-27 12:35:49" itemprop="dateCreated datePublished" datetime="2020-02-27T12:35:49-05:00">2020-02-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-17 11:19:28" itemprop="dateModified" datetime="2020-03-17T11:19:28-04:00">2020-03-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<blockquote>
<p>Refer from <em>Reinforcement Learning An Introduction (2nd)</em> and <em>Reinforcement Learning specialization on Coursera by University of Alberta &amp; Alberta Machine Intelligence Institute</em></p>
</blockquote>
<h2 id="The-K-Armed-Bandit-Problem"><a href="#The-K-Armed-Bandit-Problem" class="headerlink" title="The K-Armed Bandit Problem"></a>The K-Armed Bandit Problem</h2><p>In the k-armed bandit problem, we have an <strong>agent</strong> who choooses between “k” <strong>action</strong> and recieves a <strong>reward</strong> based on the action it chooses. The <strong>goal</strong> of the agent is to maximize the expected total reward over some time period, or <strong>time steps</strong>.</p>
<p>In a word, <strong>decision making under uncertainty</strong> can be formalized by the <strong>k-armed bandit problem</strong></p>
<h3 id="Action-Values"><a href="#Action-Values" class="headerlink" title="Action Values"></a>Action Values</h3><p>the value of an action is the <strong>expected reward</strong> when that action is taken<br>$$<br>\begin{eqnarray*}<br>q_*(a) &amp;\doteq&amp; \mathbb{E}[R_t\ | \ A_t = a],\ \forall a \in \mathbf{A}\<br>&amp;=&amp;\sum_rp(r|a)r\tag{1.1}<br>\end{eqnarray*}<br>$$<br>To clearly explain this, the reward of an action may not be fixed so the reward may comfirm to a uniform distribution or normal distribution. Here is an example to illustrate this process</p>
<img src="/2020/02/27/RL1/1.png" class="">  

<h3 id="Action-Selection"><a href="#Action-Selection" class="headerlink" title="Action Selection"></a>Action Selection</h3><p>Remember that the agent’s <strong>goal</strong> is to <strong>maximize the expected total amount of reward it recieves</strong>. We can call this procedure the <em>argmax</em> (the argument “a” which maximizes function $q_*$)<br>$$\mathop{argmax}<em>a \ q</em>*(a)$$<br>It is clear that if you know the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the <strong>action with highest value</strong>.</p>
<p>Thus, the problem here is that the agent do not know the action values with certainty. It only have to estimate the action values. But how to estimate action-values?</p>
<hr>
<h2 id="What-to-Learn-Estimating-Action-Values"><a href="#What-to-Learn-Estimating-Action-Values" class="headerlink" title="What to Learn? Estimating Action Values"></a>What to Learn? Estimating Action Values</h2><h3 id="Sample-Average"><a href="#Sample-Average" class="headerlink" title="Sample-Average"></a>Sample-Average</h3><p>One way to estimate $q_*$ is to compute a <strong>sample-average</strong>. Here we denote the estimated value of action $a$ at time step $t$ as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q_*(a)$.</p>
<p>$$Q_t(a) \doteq \frac{\sum_{i=1}^{t-1}R_i}{t-1}\tag{1.2}$$</p>
<p>Notice that we use $t-1$ because we are calculating $Q_t(a)$ with history prior to $t$. And to be clear, $t$ has <strong>separate corresponding value for each action</strong>.<br>Here is an example of medical trail.</p>
<img src="/2020/02/27/RL1/2.png" class="">  
<p>And we can simplify $Q_t$ by extending it in <strong>incremental update rule</strong></p>
<p>$$<br>\begin{eqnarray*}<br>Q_{t+1}<br>&amp;=&amp;\frac{1}{t}\sum_{i=1}^tR_i\<br>&amp;=&amp;\frac{1}{t}(R_t+(t-1)\frac{1}{t-1}\sum_{i=1}^{t-1}R_i)\<br>&amp;=&amp;\frac{1}{t}(R_t+(t-1)Q_t)\<br>&amp;=&amp;Q_t+\frac{1}{t}(R_t-Q_t)\tag{1.3}<br>\end{eqnarray*}<br>$$</p>
<p>To generalize it, we replace $\frac{1}{t}$ with $\alpha_t$ to represent stepsize and it’s easy to know $\alpha_t \in [0,1]$</p>
<p>$$Q_{t+1}=Q_t+\alpha_t(R_t-Q_t)\tag{1.4}$$</p>
<p>And it can also express as below</p>
<p>$$NewEstimate \leftarrow OldEstimate\ +\  StepSize(Target\ -\ OldEstimate)$$</p>
<p>The expression $(Target-OldEstimate)$ is an <strong>error</strong> in the estimate. It is reduced by taking a step toward the “Target.” <strong>The target is presumed to indicate a desirable direction in which to move, though it may be noisy.</strong> In the case above, for example, the target is the nth reward.</p>
<h3 id="Nonstationary-Problem"><a href="#Nonstationary-Problem" class="headerlink" title="Nonstationary Problem"></a>Nonstationary Problem</h3><p>What if the reward distribution of each action is <strong>changing over time</strong>? In this situation, what we learn form history may not correctly represent current state due to changing of reward distribution. In such cases it makes sense to give more weight to recent rewards than to long-past rewards because recent information reflect changing more accurate. To do so, we can use a <strong>fixed stepsize</strong>,like 0.5, instead of $\frac{1}{t}$. This can lead to decaying exponently past reward.</p>
<p>$$Q_{t+1}=Q_t+\alpha_t(R_t-Q_t)=\alpha_tR_t+(1-\alpha_t)Q_t\tag{1.5}$$</p>
<p>As $\alpha_t$ increases, the more $R_t$ contributes to $Q_{t+1}$ and the less for $Q_t$.</p>
<h3 id="Greedy-Action"><a href="#Greedy-Action" class="headerlink" title="Greedy Action"></a>Greedy Action</h3><p>At each time step, the agent could alway choose the action with the highest action value estimated by history. And this is called <strong>greedy action</strong>.</p>
<p>$$a_g = \mathop{argmax}_a\ \ Q(a)$$</p>
<p>Seemingly we can through greedy action to archieve our goal, maxmizing expected total reward. But there is still a problem exists: how does the agent know its estimation about each action is correctly approximating to the real distribution? Greedy action is greatly impact by initial action value. Why does the agent know other actions are worse than greedy action even trying them only a few times?</p>
<p>I think the agent should spend some time trying other actions instead of greedy action. And this come to the <strong>exploration and exploitation tradeoff</strong></p>
<hr>
<h2 id="Exploration-vs-Exploitation-Tradeoff"><a href="#Exploration-vs-Exploitation-Tradeoff" class="headerlink" title="Exploration vs. Exploitation Tradeoff"></a>Exploration vs. Exploitation Tradeoff</h2><h3 id="Exploration-and-Exploitation-Dilemma"><a href="#Exploration-and-Exploitation-Dilemma" class="headerlink" title="Exploration and Exploitation Dilemma"></a>Exploration and Exploitation Dilemma</h3><ul>
<li>Exploration: <strong>improve</strong> knowledge for <strong>long-term</strong> benefit</li>
<li>Exploitation: <strong>exploit</strong> knowledge for <strong>short-term</strong> benefit</li>
</ul>
<p>The dilemma means: How do we choose when to exploit and when to explore? When we <strong>explore</strong>, we get more accurate estimates of our values. When we <strong>exploit</strong>, we might get more reward. We cannot however choose to do both simultaneously.</p>
<h3 id="Epsilon-Greedy-Action-Selection"><a href="#Epsilon-Greedy-Action-Selection" class="headerlink" title="Epsilon-Greedy Action Selection"></a>Epsilon-Greedy Action Selection</h3><p>One very simple method for choosing between exploration and exploitation is to <strong>choose randomly</strong>. We could choose to exploit most of the time with a small chance of exploring. For instance, we could roll a dice.  </p>
<p>This is called <strong>$\epsilon$-greedy action selection</strong>. We formulize it as below, where $\epsilon \in [0,1]$</p>
<p>$$<br>A_t\leftarrow<br>\begin{cases}<br>\mathop{argmax}_a Q_t(a)\ \ \ \ \ \ \ \ 1-\epsilon\ \<br>a\sim Uniform(\mathbf{A})\ \ \ \ \ \ \epsilon<br>\end{cases}<br>$$</p>
<img src="/2020/02/27/RL1/3.png" class="">  

<p>Let’s evaluate on 10-armed testbed  </p>
<img src="/2020/02/27/RL1/5.png" class="">  
<p>We run each agent with different $\epsilon$ for 2000 independent runs, each run for 1000 time steps. Then we average 2000 runs to get the learning algorithm’s average behavior.  </p>
<img src="/2020/02/27/RL1/4.png" class="">  

<p>The advantage of $\epsilon$-greedy over greedy methods depends on the task. If the reward variance is large, like 10, then more explorations to detect best action is important. More detections give the agent more confidence to pick up optimal action.</p>
<p>On the other hand, if the reward variance is zero, then greedy method would know the true value of each action after trying it once. Seemingly, in this case, greedy method would soon find the greedy action and it might perform best. But, this can not be guaranteed under nonstationary situations. Nonstation means the true values of the actions changed over time. In this case, exploration is needed to ensure that one of the nongreedy actions has not changed to become better than the greedy one.</p>
<h3 id="Optimistic-Initial-Values"><a href="#Optimistic-Initial-Values" class="headerlink" title="Optimistic Initial Values"></a>Optimistic Initial Values</h3><p><strong>Optimistic initial values</strong> encourage <strong>early exploration</strong>. In real programing, don’t intialize all acion values defaut to 0. Here is an example to show algorithm’s performance by using this method.  </p>
<img src="/2020/02/27/RL1/6.png" class="">  
<p>Limitation of optimistic inital values:</p>
<ul>
<li>Optimistic intial values <strong>only drive early exploration</strong></li>
<li>They are not will suited for <strong>nonstationary problems</strong></li>
<li>We may not know what the <strong>optimistic values</strong> should be</li>
</ul>
<h3 id="Upper-Confidence-Bound-UCB-Action-Selection"><a href="#Upper-Confidence-Bound-UCB-Action-Selection" class="headerlink" title="Upper-Confidence Bound (UCB) Action Selection"></a>Upper-Confidence Bound (UCB) Action Selection</h3><p>$\epsilon$-greedy action selection forces the non-greedy actions to be tried, but indiscriminately and <strong>uniformly</strong>, with no preference for those that are nearly greedy or particularly uncertain. It would be better to select among the non-greedy actions according to their <strong>potential for actually being optimal</strong>, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates.</p>
<p>In <strong>upper-confidence bound</strong>(UCB) action selection, we follow the principle of optimism in the face of uncertainty. This simply means that if we are uncertain about something, we should optimistically assume that it is good.</p>
<p>For instance, say we have these three actions with associated uncertainties. The larger confidence interval means bigger uncertainies. So the agent optimistically picks the action that has the highest upper bound, action 1.</p>
<img src="/2020/02/27/RL1/7.png" class="">  
<p>After that, this time, we should select action 2.  </p>
<img src="/2020/02/27/RL1/8.png" class="">  
<p>The key idea in UCB is that it uses <strong>uncertainty</strong> or <strong>variance</strong> (sqrt root) in the value estimation for balancing exploration and exploitation.</p>
<p>$$A_t \doteq \mathop{argmax}_a [Q_t(a)+c\sqrt{\frac{\ln{t}}{N_t(a)}}\ ]$$</p>
<p>We can clearly see here how UCB combines exploration and exploitation. The first term in the sum represents the exploitation part, and the second term represents the exploration part. The constant parameter $c$ is used to scale the extent of exploration. $N_t(a)$ denotes the number of times that action $a$ has been selected prior to time $t$.  </p>
<img src="/2020/02/27/RL1/9.png" class="">  
<p>The limitation of UCB is <strong>not practical for very huge state spaces</strong>.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>The gap between k-armed bandit problem and the full reinforcement learning problem:</p>
<ol>
<li>there is <strong>no association</strong> between actions and situations(or <strong>states</strong>). Each selection of actions is <strong>non-contextual</strong> or independent.</li>
<li>thers is <strong>no policy</strong>, which designate actions at each situation(or <strong>state</strong>).</li>
</ol>
<p>we summarize a complete learning curve by its average value over the 1000 steps; this value is proportional to the area under the learning curve. This kind of graph is called a <strong>parameter study</strong>. </p>
<img src="/2020/02/27/RL1/10.png" class="">

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2020/02/25/Game2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/25/Game2/" class="post-title-link" itemprop="url">博弈论2：纯策略纳什均衡</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-25 11:23:24" itemprop="dateCreated datePublished" datetime="2020-02-25T11:23:24-05:00">2020-02-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-21 21:43:02" itemprop="dateModified" datetime="2020-03-21T21:43:02-04:00">2020-03-21</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Game-Theory/" itemprop="url" rel="index"><span itemprop="name">Game Theory</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

<blockquote>
<p>All materials come from <em>Coursera</em>《Game Theory》by Stanford University &amp; The University of British Columbia</p>
</blockquote>
<h2 id="Intro-to-nash-equilibrium"><a href="#Intro-to-nash-equilibrium" class="headerlink" title="Intro to nash equilibrium"></a>Intro to nash equilibrium</h2><p>Attributes of nash equilibrium</p>
<ul>
<li><strong>nobody has an incentive to <strong>deviate</strong> from their actions if a nash equilibrium is achieved</strong></li>
<li>each player’s action <strong>maximizes his/her payoff</strong> given the actions of others</li>
<li>a self-consistent or <strong>stable</strong> profile</li>
</ul>
<h2 id="Pure-Strategy-Nash-Equilibrium"><a href="#Pure-Strategy-Nash-Equilibrium" class="headerlink" title="Pure Strategy Nash Equilibrium"></a>Pure Strategy Nash Equilibrium</h2><h3 id="Pure-Strategy-And-Mixed-Strategy"><a href="#Pure-Strategy-And-Mixed-Strategy" class="headerlink" title="Pure Strategy And Mixed Strategy"></a>Pure Strategy And Mixed Strategy</h3><p>the informal definitions of <strong>pure strategy</strong> and <strong>mixed strategy</strong> are</p>
<ul>
<li>pure strategy: <strong>only one</strong> action is played with positive probability</li>
<li>mixed strategy: <strong>more than one</strong> action is played with positive probability</li>
</ul>
<p>With a pure strategy, a player choose one action with 100% ensurance. While with a mixed strategy, a player can have a probability of 50% choosing action A and a probability of 50% choosing action B. In another word, player $i$ chooses action $a_i$ randomly from $A$ with corresponding probability.</p>
<h3 id="Best-Response"><a href="#Best-Response" class="headerlink" title="Best Response"></a>Best Response</h3><p>The idea of <strong>Best Response</strong>: if you know what everyone else was going to do, it would be easy to pick your own action.</p>
<p>Based on the previous lecture, $a$ represents <strong>action profile</strong>, which organized by $n$ players’ action</p>
<p>$$a = (a_1, a_2,…, a_n)$$</p>
<p>Now we denote $a_{-i}$, which means a sub action profile of other players act</p>
<p>$$a_{-i} = (a_1, a_2,…, a_{i-1}, a_{i+1},…, a_n)$$</p>
<p>Then we can re-define $a$ as</p>
<p>$$a = (a_i, a_{-i})$$</p>
<p>Therefore, the definition of <strong>Best Response</strong> is showen below</p>
<p>$$a_i^* \in BR(a_{-i}) \ \ iff \ \ \forall a_i \in A_i, u_i(a_i^*, a_{-i}) \ge u_i(a_i, a_{-i})$$  </p>
<p>That is to say, given $a_{-i}$, $a_i^*$ is the best response for player $i$ to play.</p>
<p>But in practice, no agent knows what the others will do, so we can’t say about what actions will actually occur. The idea here is to look for <strong>stable action profiles</strong>, leading to equilibrium point. $a = (a_1, a_2,…, a_n)$ is a (pure strategy) <strong>nash equilibrium</strong> iff $\forall i,\ a_i \in BR(a_{-i})$</p>
<h3 id="Dominant-Strategy"><a href="#Dominant-Strategy" class="headerlink" title="Dominant Strategy"></a>Dominant Strategy</h3><p>generalize from actions to strategies</p>
<h2 id="Mixed-Strategy-Nash-Equilibrium"><a href="#Mixed-Strategy-Nash-Equilibrium" class="headerlink" title="Mixed Strategy Nash Equilibrium"></a>Mixed Strategy Nash Equilibrium</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2020/02/05/Others4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/05/Others4/" class="post-title-link" itemprop="url">Linux下常用指令</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-05 14:46:11" itemprop="dateCreated datePublished" datetime="2020-02-05T14:46:11-05:00">2020-02-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-29 04:03:22" itemprop="dateModified" datetime="2020-02-29T04:03:22-05:00">2020-02-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Others/" itemprop="url" rel="index"><span itemprop="name">Others</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="top"><a href="#top" class="headerlink" title="top"></a>top</h2><p>查看系统进程</p>
<h2 id="screen"><a href="#screen" class="headerlink" title="screen"></a>screen</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">screen -S [TaskName] 创建一个名为TaskName的screen</span><br><span class="line">screen -r(-x) [TaskName] 恢复（进入）名为TaskName的screen</span><br></pre></td></tr></table></figure>

<p>快捷指令：</p>
<ul>
<li>上下分屏：ctrl + a  再按shift + s</li>
<li>切换屏幕：ctrl + a  再按tab键</li>
<li>新建一个终端：ctrl + a  再按c</li>
</ul>
<h2 id="du、df"><a href="#du、df" class="headerlink" title="du、df"></a>du、df</h2><p>du = disk used 硬盘使用情况<br>df = disk free 硬盘剩余空间</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">du -h [FileName] 查看指定文件大小</span><br><span class="line"></span><br><span class="line">df -hl 查看磁盘剩余空间</span><br><span class="line"></span><br><span class="line">df -h 查看每个根路径的分区大小</span><br><span class="line"></span><br><span class="line">du -sh [目录名] 返回该目录的大小</span><br><span class="line"></span><br><span class="line">du -sm [文件夹] 返回该文件夹总M数</span><br></pre></td></tr></table></figure>

<h2 id="find"><a href="#find" class="headerlink" title="find"></a>find</h2><p>find命令是根据文件的属性进行查找，如文件名，文件大小，所有者，所属组，是否为空，访问时间，修改时间等</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find [location] -name [FileName] 在location下查找名字为FileName的文件</span><br></pre></td></tr></table></figure>

<h2 id="grep"><a href="#grep" class="headerlink" title="grep"></a>grep</h2><p>grep是根据文件的内容进行查找，会对文件的每一行按照给定的模式(patter)进行匹配查找</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep [字符串] [File] 在File中按行匹配字符串</span><br></pre></td></tr></table></figure>

<h2 id="which、whereis"><a href="#which、whereis" class="headerlink" title="which、whereis"></a>which、whereis</h2><p>which 查看可执行文件的位置 ，只有设置了环境变量的程序才可以用<br>whereis 寻找特定文件，只能用于查找二进制文件、源代码文件和man手册页</p>
<h2 id="more-less"><a href="#more-less" class="headerlink" title="more/less"></a>more/less</h2><p>查看文件内容，当文件内容很大时比cat命令要好用</p>
<h2 id="head、tail"><a href="#head、tail" class="headerlink" title="head、tail"></a>head、tail</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">head(tail) -n 1000 [FileName] 显示FileName文件的前1000行(后1000行)</span><br><span class="line">tail -n +1000 [FileName] 显示FileName文件从1000行往后的内容</span><br><span class="line">tail -f [FileName] 追踪（follow）文件FileName</span><br></pre></td></tr></table></figure>

<h2 id="free、swapoff、mkswap、swapon"><a href="#free、swapoff、mkswap、swapon" class="headerlink" title="free、swapoff、mkswap、swapon"></a>free、swapoff、mkswap、swapon</h2><p>查看内存状态命令，可以显示memory、swap、buffer/cache等的大小及使用状况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">free -h 查看内存状态</span><br><span class="line"></span><br><span class="line">swapoff -a 取消所有挂载的swap空间</span><br><span class="line"></span><br><span class="line">dd if=/dev/zero of=/var/swapfile bs=1M count=1024 在/var目录下创建大小为1G的文件用做swap空间</span><br><span class="line"></span><br><span class="line">mkswap /var/swapfile 格式化swap文件</span><br><span class="line"></span><br><span class="line">swapon /var/swapfile 挂载swap空间</span><br></pre></td></tr></table></figure>

<h2 id="netstat"><a href="#netstat" class="headerlink" title="netstat"></a>netstat</h2><p>查看网络状态（地址和端口号）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -tunlp 其中t（tcp）、u（udp），nlp分别是三个flag参数</span><br></pre></td></tr></table></figure>

<h2 id="tmux"><a href="#tmux" class="headerlink" title="tmux"></a>tmux</h2><p><a target="_blank" rel="noopener" href="http://www.ruanyifeng.com/blog/2019/10/tmux.html">详细内容参考</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tmux new -s [NewSessionName] 创建标签为NewSessionName的新会话，默认数字index从0开始</span><br><span class="line">tmux attach -t [index/name] 恢复到index/name的会话连接</span><br><span class="line">tmux ls 查看所有会话</span><br><span class="line">tmux switch -t [index/name] 切换到index/name会话</span><br></pre></td></tr></table></figure>

<p>快捷指令  ：</p>
<ul>
<li>ctrl + b %： 左右分屏</li>
<li>ctrl + b “： 上下分屏</li>
<li>ctrl + b 空格： 左右分屏</li>
<li>ctrl + b 上下左右： 切换分屏</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2020/01/20/Game1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/20/Game1/" class="post-title-link" itemprop="url">博弈论1：为什么囚徒困境是一个困境?</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-20 19:48:16" itemprop="dateCreated datePublished" datetime="2020-01-20T19:48:16-05:00">2020-01-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-06 20:56:51" itemprop="dateModified" datetime="2020-03-06T20:56:51-05:00">2020-03-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Game-Theory/" itemprop="url" rel="index"><span itemprop="name">Game Theory</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>All materials come from <em>Coursera</em>《Game Theory》by Stanford University &amp; The University of British Columbia</p>
</blockquote>
<h2 id="囚徒困境"><a href="#囚徒困境" class="headerlink" title="囚徒困境"></a>囚徒困境</h2><p>两个囚犯现在被捕，警察将他们两个分别关在了不同的房间（囚犯之间互相无法交谈），警察为了让他们招供，对两个人分别说了这样一番话：</p>
<ol>
<li>若你和他都选择不招供，则在我们目前已有证据的基础上，我们有信心将你们<strong>两个都判1年</strong></li>
<li>若你选择招供告发他，而他不招供，那么我们将<strong>直接无罪释放你并且他会被判3年</strong></li>
<li>若你和他都选择了招供，那问题就很简单了，你们将<strong>都被判2年</strong></li>
</ol>
<p>现在我们将上述规则以更简练的矩阵形式表示出来  </p>
<img src="/2020/01/20/Game1/pic1.png" class="">  
<p>其中C表示囚犯选择与对方合作（Cooperate）即<strong>不招供</strong>，D表示囚犯选择欺骗对方（Defect）即选择<strong>招供</strong>，左边的C和D代表囚犯1可以做的两个选择，同理上方的C和D代表囚犯2可以做的两个选择。举个例子，对于<strong>右上角</strong>的方格，表示<strong>囚犯1选择不招供(C)而囚犯2选择招供(D)<strong>，结果</strong>囚犯1被判了3年而囚犯2无罪释放</strong></p>
<p>为了进一步说明这个问题，我们先引入一些博弈论的基本概念</p>
<hr>
<h2 id="博弈的基本定义"><a href="#博弈的基本定义" class="headerlink" title="博弈的基本定义"></a>博弈的基本定义</h2><p>博弈的基本元素有以下几个：</p>
<ul>
<li>玩家（Players）：做出决定的个体</li>
<li>行动（Actions）：每个玩家可以做出怎样的行动</li>
<li>奖励/代价（Payoff）：玩家做出每个行动之后会有怎样反馈（例如商人在股票最高点抛售这一行动可以获得最高的收益）</li>
<li>时序（Timing）：事件发生的顺序（例如大家下棋时轮流交替移动棋子）</li>
<li>信息（information）：当玩家做出行动时他们都知道什么信息（参与者对其他参与者的了解程度）</li>
</ul>
<p>所以<strong>有限n人标准</strong>博弈的数学定义表示为$&lt;N, \mathbf{A}, u&gt;$其中元素分别表示如下含义：</p>
<ul>
<li>(1) 玩家的集合$N = {1, 2, …, n}$ 是一个大小为n的有限集合，用$i$标识</li>
<li>(2) 行动组合的全集$\mathbf{A} = A_1 \times A_2 \times … \times A_n$，其中$A_i$表示玩家$i$的行动集合，若用$a$表示一个行动组合（action profile），则有$a = (a_1, a_2, …, a_n) \in \mathbf{A}$</li>
<li>(3) 效用(代价)函数组$u = (u_1, u_2, …, u_n)$，其中对于玩家$i$，$u_i: \mathbf{A} \rightarrow \mathbb{R}$，特别注意 ，此处的$u_i$表示一个从行动组合集合$a \in A$到实数$\mathbf{R}$的映射</li>
</ul>
<hr>
<h2 id="支配策略"><a href="#支配策略" class="headerlink" title="支配策略"></a>支配策略</h2><p>我们在参与游戏的时会有一个要实现的<strong>目标</strong>（goal），通常情况下的目标为最大化自己的收益（payoff），但是也可以有别的目标（比如以让对方获得最小收益为目标）</p>
<p>为实现这个目标我们会有一个对应的<strong>策略</strong>（strategy），<strong>策略</strong>指导玩家该做出什么行动（或选择什么行动）。比如在囚徒困境中，囚犯的策略就是：<strong>做出怎样的选择(招供或不招供)，以使自己被判的时间最短</strong>。这里每个囚犯有两个选择：与另一个囚犯合作（Cooperate）和欺骗另一个囚犯（Deceive），选择合作意味着<strong>不招供</strong>而欺骗意味着<strong>招供</strong>，所以每个囚犯都会思考，如果对方招供的话我该怎么选，如果对方不招供的话我又该怎么选？从而我们这里引入一个概念<strong>支配策略</strong>（dominant strategy）</p>
<p>支配策略：A dominant strategy for a player is one that produces the highest payoff of any strategy available for every possible action by the other players。通俗点讲就是：不管别人做出怎样的行动，我所做的选择都可以让我得到最大收益。还以囚徒困境举例  </p>
<p>情况一：当<strong>囚犯2选择C不招供</strong>（下图红框），那么对于囚犯1来说：选C不招供那么自己会被判1年，而D招供那么自己可以无罪释放（下图绿框）。所以很显然，<strong>此时选D招供是最好的选择</strong></p>
<img src="/2020/01/20/Game1/pic2.png" class="">  

<p>情况二：当<strong>囚犯2选择D招供</strong>（下图红框），那么对于囚犯1来说：选C不招供那么自己会被判3年，而D招供那么自己会被判2年（下图绿框）。所以很显然，<strong>此时选D招供是最好的选择</strong></p>
<img src="/2020/01/20/Game1/pic3.png" class="">  

<p>综合上述两种情况，我们可以得出结论：<strong>不论囚犯2如何选择，囚犯1的最佳选择都是D招供</strong>（也就是囚犯1的支配策略）。同样的道理，由于囚犯1和囚犯2的地位是<strong>对称的</strong>，所以<strong>囚犯2的支配策略也是D招供</strong></p>
<h2 id="关于囚徒困境的结论"><a href="#关于囚徒困境的结论" class="headerlink" title="关于囚徒困境的结论"></a>关于囚徒困境的结论</h2><p>根据之前的支配策略的分析，两个囚犯的支配策略都是D招供，从而两个人<strong>都被判2年</strong>。而囚徒困境的令人诧异的地方也就出现于此，我们如果从一个上帝视角来看，很明显，两个囚犯的最佳选择明明是选择都不招供，这样两个人加起来判的时间是最少的，是全局最优解，但是如果从个人角度来分析，就会发现，无论如何他们两个都不会去选择一起合作不招供。这恰恰就是囚徒困境的背景————两个人无法交流即非合作博弈，<strong>其所谓的上帝视角其实是不存在的</strong>，因为将其推广到繁杂的现实，就会发现没有“上帝”可以看到整个全局，没有人的信息是全局的，从而从这个思想出发，反驳了亚当斯密的一个基本观点:所有人的最优选择会形成整个群体的最优选择，因为当所有人做到自己的最优时，对于整个社会来说可能是最坏的（两个囚犯加起来一共判了4年）</p>
<h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>囚徒困境的标准通式满足：<br>$$c &gt; a &gt; d &gt; b$$</p>
<img src="/2020/01/20/Game1/pic4.png" class="">

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2020/01/17/ML13/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/01/17/ML13/" class="post-title-link" itemprop="url">统计学习方法——第十三章：无监督学习概论</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-01-17 21:55:39" itemprop="dateCreated datePublished" datetime="2020-01-17T21:55:39-05:00">2020-01-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-01-20 08:44:53" itemprop="dateModified" datetime="2020-01-20T08:44:53-05:00">2020-01-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p><em>参考书目《统计学习方法（第2版）》 李航 著</em></p>
</blockquote>
<h2 id="无监督学习基本原理"><a href="#无监督学习基本原理" class="headerlink" title="无监督学习基本原理"></a>无监督学习基本原理</h2><ul>
<li>无监督学习是从无标注的数据中学习数据的统计规律或者说内在结构的机器学习，主要包括<strong>聚类、降维、概率估计</strong></li>
<li>无监督学习的基本想法是对给定数据（矩阵数据）进行某种“压缩”，从而找到数据的潜在结构。<strong>假定损失最小的压缩得到的结果就是最本质的结构</strong></li>
</ul>
<p>发掘数据的<em>纵向结构</em>，把相似的样本聚到同类，即对数据进行<strong>聚类</strong>  </p>
<img src="/2020/01/17/ML13/pic1.png" class="">  
<p>发掘数据的<em>横向结构</em>，把高维空间的向量转换为低维空间的向量，即对数据进行<strong>降维</strong>聚类  </p>
<img src="/2020/01/17/ML13/pic2.png" class="">  
<p>同时发掘数据的纵向和横向结构，假设数据由含有隐式结构的概率模型生成得到，从数据中<strong>学习该概率模型</strong>  </p>
<img src="/2020/01/17/ML13/pic3.png" class="">  

<hr>
<h2 id="三个基本问题"><a href="#三个基本问题" class="headerlink" title="三个基本问题"></a>三个基本问题</h2><h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h3><ul>
<li>聚类（clustering）是将样本集合中相似的样本（实例）分配到相同的类，不相似的样本分配到不同的类。</li>
<li>聚类时，样本通常是欧氏空间中的向量，类别不是预先给定，而是从数据中自动发现，但类别的个数通常是预先给定的。样本之间的相似度或距离由 应用决定。</li>
<li>如果一个样本只能属于一个类，则称为硬聚类（硬聚类）<br>  $$z_i = g_\theta(x_i), i = 1,2,…,N$$</li>
<li>如果一个样本可以属于多个类，则称为软聚类（软聚类）<br>  $$P_\theta(z_i|x_i), i = 1,2,…,N$$</li>
</ul>
<h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><ul>
<li>降维（dimensionality reduction）是将训练数据中的样本（实例）从高维空间转换到低维空间</li>
<li>假设样本原本存在于低维空间，或者近似地存在于低维空间，通过降维则可以更好地表示样本数据的结构，即更好地表示样本之间的关系</li>
<li>高维空间通常是高维的欧氏空间，而低维空间是低维的欧氏空间或者流形（manifold)</li>
<li>从高维到低维的降维中，要保证样本中的信息损失最小</li>
</ul>
<img src="/2020/01/17/ML13/pic4.png" class="">

<p>降维的模型是函数<br>$$z_i = g_\theta(x_i), i = 1,2,…,N$$<br>其中$x_i \in X$是样本的高维向量，$z_i \in Z$是样本的低维向量，$\theta$是参数，函数可以是线性函数也可以是非线性函数，降维的过程就是学习降维模型的过程</p>
<h3 id="概率模型估计"><a href="#概率模型估计" class="headerlink" title="概率模型估计"></a>概率模型估计</h3><ul>
<li>概率模型估计（probility model estimation），简称概率估计，假设训练数据由一个概率模型生成，由训练数据学习概率模型的结构和参数。</li>
<li>概率模型的<strong>结构类型</strong>（或者说概率模型的集合事先给定），而模型的<strong>具体结构与参数从数据中自动学习</strong>。学习的目标是找到最有可能生成数据的结构和参数。</li>
</ul>
<img src="/2020/01/17/ML13/pic5.png" class="">

<p>概率模型表示为条件概率分布<br>$$P_\theta(x|z)$$<br>随机变量$x$表示观测数据，可以是连续变量也可以是离散变量，随机变量$z$表示隐式结构，是离散变量，随机变量$\theta$表示参数。概率模型的一种特殊情况是隐式结构不存在，即满足<br>$$P_\theta(x|z) = P_\theta(x)$$<br>这时条件 概率分布估计变成概率分布估计，只要估计分布$P_\theta(x)$的参数即可</p>
<hr>
<h2 id="无监督学习三要素"><a href="#无监督学习三要素" class="headerlink" title="无监督学习三要素"></a>无监督学习三要素</h2><ul>
<li>(1)模型:<br>  函数$$z = g_\theta(x)$$，条件概率分布$$P_\theta(z|x)$$，或条件概率分布$$P_\theta(x|z)$$</li>
<li>(2)策略:<br>  目标函数的优化</li>
<li>(3)算法:<br>  迭代算法，通过迭代达到对目标函数的最优化</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2019/11/25/GPU1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/25/GPU1/" class="post-title-link" itemprop="url">Nvidia GPU 基本概念</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-11-25 08:46:56" itemprop="dateCreated datePublished" datetime="2019-11-25T08:46:56-05:00">2019-11-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-11-24 21:36:25" itemprop="dateModified" datetime="2019-11-24T21:36:25-05:00">2019-11-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GPU/" itemprop="url" rel="index"><span itemprop="name">GPU</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Physical-Architecture-of-GPU"><a href="#Physical-Architecture-of-GPU" class="headerlink" title="Physical Architecture of GPU"></a>Physical Architecture of GPU</h2><p>下图为Nvidia官网上找的一个GPU主板的概念图，对应Device</p>
<ul>
<li>Device：与host做区分，可以理解为是指整个显卡</li>
</ul>
<img src="/2019/11/25/GPU1/pic2.png" class="">
<p>如下图所示（Nvidia Tesla P100），共包含60个SM（6GPC x 10SM）</p>
<ul>
<li>SM(Streaming Multiprocessor)：等价于NUMA中的core的概念，由多个SP加上其他的一些硬件资源组成<ul>
<li>其他硬件资源如：warp scheduler，register，shared memory等</li>
<li>同一个block中的warp一定分配给同一个SM</li>
<li>每个SM有自己独立的local L1 cache，但所有SM会共用Device的L2 cache</li>
</ul>
</li>
</ul>
<img src="/2019/11/25/GPU1/pic3.png" class="">

<p>每个SM有自己的内部结构，如下图所示GP100（Nvidia Tesla P100）中的SM内部结构，在GP100里，每一个SM有左右两个SM Processing Block（SMP）</p>
<img src="/2019/11/25/GPU1/pic4.png" class="">

<ul>
<li>SP(Streaming Processor)：最基本的处理单元，也称为CUDA core，表示在上图中为绿色的小格子<ul>
<li>相当于一个简易的CPU</li>
<li>内部包括控制单元Dispatch Port、Operand Collector，以及浮点计算单元FP Unit、整数计算单元Int Unit，另外还包括计算结果队列。当然还有Compare、Logic、Branch等</li>
</ul>
</li>
</ul>
<p>下图为SP（CUDA core）的内部结构</p>
<img src="/2019/11/25/GPU1/pic6.png" class="">

<h2 id="Software-Concept-in-CUDA"><a href="#Software-Concept-in-CUDA" class="headerlink" title="Software Concept in CUDA"></a>Software Concept in CUDA</h2><ul>
<li>Thread：一个CUDA的并行程序会被分配到多个threads来执行<ul>
<li>一个SP对应执行一个thread</li>
<li>threadId在cuda中为三维的概念，在一个block中由(x, y, z)来唯一定位</li>
</ul>
</li>
<li>Block：多个thread群组成一个block<ul>
<li>同一个block中的threads可以同步，也可以通过shared memory通信</li>
<li>blockId在cuda中为二维的概念，在一个grid中由(x, y)来唯一定位</li>
</ul>
</li>
<li>Grid：多个block构成grid<ul>
<li>一个grid对应一个执行一个kernel函数</li>
</ul>
</li>
</ul>
<p>空间对应如下图所示  </p>
<img src="/2019/11/25/GPU1/pic1.png" class="">

<ul>
<li>warp：执行任务的调度单元，同一时间一个SM只能运行一个warp<ul>
<li>类比于CPU中的进程，当一个warp阻塞时，SM将切换执行另一个warp</li>
<li>当前CUDA的warp大小为32个线程。这里表明Nvidia的GPU执行粒度小，执行线程粒度越小，并行度越高，掩盖延迟的能力越强</li>
<li>同在一个warp中的线程，以不同数据资源执行相同的指令，即所谓的SIMT（Single Instruction Multiple Thread）</li>
</ul>
</li>
</ul>
<h2 id="CPU和GPU的结构对比"><a href="#CPU和GPU的结构对比" class="headerlink" title="CPU和GPU的结构对比"></a>CPU和GPU的结构对比</h2><img src="/2019/11/25/GPU1/pic5.png" class="">

<ul>
<li>CPU是靠更大的缓存和复杂的逻辑控制（如流水线、乱序执行、指令预测和预处理等等）来掩饰延迟的</li>
<li>GPU是靠更高的并行度来掩饰延迟的</li>
</ul>
<h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><ul>
<li>GPU在并发的warp之间切换是没什么消耗的，因为硬件资源早就被分配到所有thread和block，所以新调度的warp的状态已经存储在SM中了。不同于CPU，CPU切换线程需要保存/读取线程上下文（register内容），这是非常耗时的，而GPU为每个threads提供物理register，无需保存/读取上下文。</li>
<li>register和shared memory是SM的稀缺资源。CUDA将这些资源在程序执行前就分配给所有驻留在SM中的warp，并且整个程序执行周期都不再变动。因此这些有限的资源就会限制每个SM中active warp的数量，实验结果显示一个SM至少要存在6个active warp才能有效掩饰延迟</li>
</ul>
<h2 id="Reference-Material"><a href="#Reference-Material" class="headerlink" title="Reference Material"></a>Reference Material</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/junparadox/article/details/50540602">CUDA编程——GPU架构，由sp，sm，thread，block，grid，warp说起</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/fpgatom/article/details/82081422">GPU高性能编程CUDA实战</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/asasasaababab/article/details/80447254">Nvidia GPU架构 - Cuda Core，SM，SP</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/junparadox/article/details/50540602">CUDA编程——GPU架构，由sp，sm，thread，block，grid，warp说起</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41598072/article/details/82877655">CUDA中grid、block、thread、warp与SM、SP的关系</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2019/07/03/Others2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/07/03/Others2/" class="post-title-link" itemprop="url">Shell中的常用通配符，字符类</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-07-03 10:42:24" itemprop="dateCreated datePublished" datetime="2019-07-03T10:42:24-04:00">2019-07-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-07-02 22:49:35" itemprop="dateModified" datetime="2019-07-02T22:49:35-04:00">2019-07-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Others/" itemprop="url" rel="index"><span itemprop="name">Others</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/hxphp/p/6707718.html">转载出处</a><br>因为 shell 频繁 地使用文件名，shell 提供了特殊字符来帮助你快速指定一组文件名。这些特殊字符叫做通配符。</p>
<table>
<thead>
<tr>
<th>通配符</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>*</td>
<td>匹配任意多个字符（包括零个或一个）</td>
</tr>
<tr>
<td>?</td>
<td>匹配任意一个字符（不包括零个）</td>
</tr>
<tr>
<td>[characters]</td>
<td>匹配任意一个属于字符集中的字符</td>
</tr>
<tr>
<td>[!characters]</td>
<td>匹配任意一个不是字符集中的字符</td>
</tr>
<tr>
<td>[[:class:]]</td>
<td>匹配任意一个属于指定字符类中的字符</td>
</tr>
</tbody></table>
<p>&emsp;</p>
<table>
<thead>
<tr>
<th>字符类</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>[:alnum:]</td>
<td>匹配任意一个字母或数字</td>
</tr>
<tr>
<td>[:alpha:]</td>
<td>匹配任意一个字母</td>
</tr>
<tr>
<td>[:digit:]</td>
<td>匹配任意一个数字</td>
</tr>
<tr>
<td>[:lower:]</td>
<td>匹配任意一个小写字母</td>
</tr>
<tr>
<td>[:upper]</td>
<td>匹配任意一个大写字母</td>
</tr>
</tbody></table>
<p>一些常用的匹配：</p>
<table>
<thead>
<tr>
<th>模式</th>
<th>匹配对象</th>
</tr>
</thead>
</table>
<ul>
<li>| 所有文件<br>g* | 文件名以“g”开头的文件<br>b*.txt 以”b” | 开头，中间有零个或任意多个字符，并以”.txt” 结尾的文件<br>Data??? | 以“Data”开头，其后紧接着 3 个字符的文件<br>[abc]* | 文件名以”a”,”b”, 或”c” 开头的文件<br>BACKUP.[0-9][0-9][0-9] | 以”BACKUP.” 开头，并紧接着 3 个数字的文件<br>[[:upper:]]* | 以大写字母开头的文件<br>[![:digit:]]* | 不以数字开头的文件</li>
<li>[[:lower:]123] | 文件名以小写字母结尾，或以“1”，“2”，或“3”结尾的文件</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="">
    <link itemprop="mainEntityOfPage" href="https://wshenyi.github.io/2019/06/17/Database11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.ico">
      <meta itemprop="name" content="Godway">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Godway's Notebook">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/17/Database11/" class="post-title-link" itemprop="url">数据库原理——第十一章：并发控制</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-06-17 19:21:04" itemprop="dateCreated datePublished" datetime="2019-06-17T19:21:04-04:00">2019-06-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2019-06-19 02:22:15" itemprop="dateModified" datetime="2019-06-19T02:22:15-04:00">2019-06-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8E%9F%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">数据库原理</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p><em>参考书目《数据库系统概论（第5版）》</em></p>
</blockquote>
<h2 id="并发控制概述"><a href="#并发控制概述" class="headerlink" title="并发控制概述"></a>并发控制概述</h2><p>并发操作带来的不一致性：</p>
<ul>
<li>丢失修改：T1和T2同时读入并修改，后者的提交覆盖了前者的提交</li>
<li>不可重复读：T1读取了数据A，期间T2修改了数据A，当T1再次读取A时发现与原先不一致</li>
<li>读“脏”数据：T1保存了对A的修改，但在T2读取A的期间，T1对A的操作被撤销导致A恢复原值，从而T2读取的是<strong>错误数据</strong></li>
</ul>
<p>并发控制的主要技术：封锁、时间戳、乐观控制法和多版本并发控制</p>
<hr>
<h2 id="封锁与封锁协议"><a href="#封锁与封锁协议" class="headerlink" title="封锁与封锁协议"></a>封锁与封锁协议</h2><p><strong>封锁</strong>就是事务在对某数据对象操作之前先向系统发出请求，对其<strong>加锁</strong><br>基本的封锁类型有：</p>
<ul>
<li><strong>排他锁</strong>又称<strong>写锁</strong>（简称X锁）</li>
<li><strong>共享锁</strong>又称<strong>读锁</strong>（简称S锁）</li>
</ul>
<p>封锁协议：</p>
<ol>
<li>一级封锁协议：事务T在<strong>修改</strong>数据R之前必须先对其加X锁，直到事务结束才释放<br>一级封锁协议可防止丢失修改，并保证事务T是可恢复的，但其不保证可重复读和不读错误数据</li>
<li>二级封锁协议：在一级封锁协议基础上，增加事务T在<strong>读取</strong>数据R之前必须先对其加S锁，<strong>读完后</strong>即可释放S锁<br>二级封锁协议除防止了丢失修改，还进一步防止读错误数据，但其不保证可重复读</li>
<li>三级封锁协议：在一级封锁协议基础上，增加事务T在<strong>读取</strong>数据R之前必须先对其加S锁，<strong>直到事务结束</strong>释放S锁<br>三级封锁协议除防止了丢失修改，读读错误数据和不可重复读</li>
</ol>
<hr>
<h2 id="活锁和死锁"><a href="#活锁和死锁" class="headerlink" title="活锁和死锁"></a>活锁和死锁</h2><p>活锁：就是操作系统中的<strong>饥饿</strong>，使用FCFS来预防活锁<br>死锁：就是操作系统中的<strong>死锁</strong><br>死锁的预防：</p>
<ul>
<li>一次封锁法：一次将所有要使用的数据全部加锁</li>
<li>顺序封锁法：先对数据对象规定一个封锁顺序，所有事务都按这个顺序实施封锁</li>
</ul>
<p>操作系统中采用的预防死锁不适合数据库，因此数据库普遍采用诊断并解除死锁的方法，即允许发生死锁， 然后采用一定手段定期诊断系统中有无死锁， 若有则解除之<br>死锁的<strong>诊断</strong>方法：</p>
<ul>
<li>超时法</li>
<li>等待图法</li>
</ul>
<p>死锁的<strong>解除</strong>，通常采用的方法是：<strong>选择一个处理死锁代价最小的事务，将其撤销，释放此事务持有的所有锁，使其他事务得以继续运行下去</strong></p>
<hr>
<h2 id="并发调度的可串行性"><a href="#并发调度的可串行性" class="headerlink" title="并发调度的可串行性"></a>并发调度的可串行性</h2><p><strong>可串行化调度</strong>：多个事务的并发执行是正确的，当且仅当其结果与按某一次序串行执行这些事务时的结果相同<br><strong>可串行性</strong>是并发事务正确调度的准则。即一个给定的并发调度，当且仅当它是可串行化的，才认为是<strong>正确调度</strong><br>若一个调度是<strong>冲突可串行化</strong>，则一定是<strong>可串行化</strong>调度  </p>
<hr>
<h2 id="两段锁协议"><a href="#两段锁协议" class="headerlink" title="两段锁协议"></a>两段锁协议</h2><p>数据库系统采用<strong>两段锁协议</strong>（2PL）的方法实现并发调度的可串行性，从而保证调度的正确性<br>两段锁协议是指所有事务必须分两个阶段对数据项加锁和解锁：</p>
<ul>
<li>扩展阶段（第一阶段）：获得封锁。在对任何数据进行读、写操作之前，首先要申请并获得对该数据的封锁；此阶段，可以申请获得任何数据项上任何类型的锁，但是不能释放锁</li>
<li>收缩阶段（第二阶段）：释放封锁。在释放一个封锁之后，事务不再申请和获得任何其他封锁</li>
</ul>
<p>事务遵守两段锁协议是可串行化调度的<strong>充分条件</strong><br>一次封锁法遵守两段锁协议，但两段锁协议并不要求事务必须一次将所有要使用的数据全部加锁，因此<strong>遵守两段锁协议的事务仍可能发生死锁</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Godway"
      src="/images/avatar.ico">
  <p class="site-author-name" itemprop="name">Godway</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">60</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="/475865836@qq.com" title="E-Mail → 475865836@qq.com"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://randool.cn/" title="https:&#x2F;&#x2F;randool.cn" rel="noopener" target="_blank">Randool</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://me.csdn.net/Smile_coderrr" title="https:&#x2F;&#x2F;me.csdn.net&#x2F;Smile_coderrr" rel="noopener" target="_blank">Smile_coderrr</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Godway</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
